{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:23:11.071711Z",
     "start_time": "2019-03-28T14:23:05.564766Z"
    }
   },
   "outputs": [],
   "source": [
    "import getopt\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "# import traceback\n",
    "import tweepy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "#import skimage.io as io  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:23:11.785556Z",
     "start_time": "2019-03-28T14:23:11.777534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate your own at https://apps.twitter.com/app\n",
    "CONSUMER_KEY = 'q8svcQ1GKW2yknY8MCZLvcO7w'\n",
    "CONSUMER_SECRET = 'kk9eMhfIMVxoDEoKR63ddWooW87Ya7IgUt5oC31S0TpAXeiMdh'\n",
    "OAUTH_TOKEN = '917762487608659970-G1v4Nr01JQA9UKqO1HP4g4bPwKT7LAr'\n",
    "OAUTH_TOKEN_SECRET = 'p1Zp4ophwRbRvR5yET3ppXWWg7fEshIyWwby9vTBxR9CF'\n",
    "\n",
    "# connect to twitter\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# batch size depends on Twitter limit, 100 at this time\n",
    "batch_size = 100\n",
    "\n",
    "#Some emojis have character length of more than 1\n",
    "emoji_threshold = 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:23:17.955554Z",
     "start_time": "2019-03-28T14:23:12.462633Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>imgid</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>742995415264546816</td>\n",
       "      <td>http://pbs.twimg.com/media/Ck80Z5TUYAAb4sM.jpg</td>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>746964081370865664</td>\n",
       "      <td>http://pbs.twimg.com/media/Cl2_3D7WkAAcNYE.jpg</td>\n",
       "      <td>1103,1108,1103,1108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>741083806547857408</td>\n",
       "      <td>http://pbs.twimg.com/media/CH1pK09UYAAhgCh.jpg</td>\n",
       "      <td>1102,1135,1138,1241,1102,1135,1241,1102,1135,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>746749796262645761</td>\n",
       "      <td>http://pbs.twimg.com/media/ClxnlSaUkAAUruc.jpg</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>744903166806786049</td>\n",
       "      <td>http://pbs.twimg.com/media/ClZteXpUgAEzdIW.jpg</td>\n",
       "      <td>820,1105,1413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                           imgid  \\\n",
       "0  742995415264546816  http://pbs.twimg.com/media/Ck80Z5TUYAAb4sM.jpg   \n",
       "1  746964081370865664  http://pbs.twimg.com/media/Cl2_3D7WkAAcNYE.jpg   \n",
       "2  741083806547857408  http://pbs.twimg.com/media/CH1pK09UYAAhgCh.jpg   \n",
       "3  746749796262645761  http://pbs.twimg.com/media/ClxnlSaUkAAUruc.jpg   \n",
       "4  744903166806786049  http://pbs.twimg.com/media/ClZteXpUgAEzdIW.jpg   \n",
       "\n",
       "                                         annotations  \n",
       "0                                                865  \n",
       "1                                1103,1108,1103,1108  \n",
       "2  1102,1135,1138,1241,1102,1135,1241,1102,1135,1...  \n",
       "3                                                186  \n",
       "4                                      820,1105,1413  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'cleaned_img_train_plaintext.txt'\n",
    "#file = 'img_train_plaintext.txt'\n",
    "data = pd.read_csv(file, sep='\\t', encoding = 'utf8', engine='c', header = 0)\n",
    "\n",
    "#data = pd.DataFrame([j for i,j in enumerate(data.values) if len(data.iloc[i, 2].split(',')) == 1], columns=['id','imgid','annotations'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:23:37.982255Z",
     "start_time": "2019-03-28T14:23:37.977241Z"
    }
   },
   "outputs": [],
   "source": [
    "def locate_emoji(emoji_pattern, text: str):\n",
    "    emoji = ''.join(emoji_pattern.findall(text))\n",
    "    try:\n",
    "        index = text.index(emoji)\n",
    "    except:\n",
    "        index = - emoji_threshold\n",
    "    return emoji, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:29:24.897136Z",
     "start_time": "2019-03-28T14:29:24.886108Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tweets(twapi, data):\n",
    "    '''\n",
    "    Fetches content for tweet IDs in a file using bulk request method,\n",
    "    which vastly reduces number of HTTPS requests compared to above;\n",
    "    however, it does not warn about IDs that yield no tweet.\n",
    "    `twapi`: Initialized, authorized API object from Tweepy\n",
    "    '''\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE)\n",
    "\n",
    "    tweet_ids = data.id.values.tolist()\n",
    "    emoji_labels = data.annotations.values.tolist()\n",
    "\n",
    "    all_tweets = []\n",
    "    labels = []\n",
    "    i = 0  #for debug\n",
    "    # process list of ids until it's empty\n",
    "    while len(tweet_ids) > 0:\n",
    "        if len(tweet_ids) < batch_size:\n",
    "            tweets = twapi.statuses_lookup(\n",
    "                id_=tweet_ids, include_entities=False, trim_user=True)\n",
    "            tweet_ids = []\n",
    "        else:\n",
    "            tweets = twapi.statuses_lookup(\n",
    "                id_=tweet_ids[:batch_size],\n",
    "                include_entities=False,\n",
    "                trim_user=True)\n",
    "            tweet_ids = tweet_ids[batch_size:]\n",
    "\n",
    "        for tweet in tweets:\n",
    "            \n",
    "            #removes the link of the tweet\n",
    "            text = re.sub(r'http\\S+', '', tweet.text).strip(' ')            \n",
    "            text = re.sub(r'\\ART @\\S+','', text).strip(' ')\n",
    "            \n",
    "            #Remove tweets where emoji is not at the end\n",
    "            emoji, index = locate_emoji(emoji_pattern, text)\n",
    "            \n",
    "            if index >= len(text) - emoji_threshold:\n",
    "                #removes the emojis from the text\n",
    "                text = emoji_pattern.sub(r'', text).strip(' ')\n",
    "\n",
    "                #then appends the tweet and emoji to our final dataset\n",
    "                all_tweets.append(np.array([text]))\n",
    "                labels.append(emoji)\n",
    "\n",
    "        i += 1\n",
    "        if i == 5:\n",
    "            break\n",
    "            \n",
    "    features = all_tweets\n",
    "    #features = np.array(all_tweets)\n",
    "    #labels = np.array(labels)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:29:29.469091Z",
     "start_time": "2019-03-28T14:29:25.994115Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = get_tweets(api, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T17:23:41.799357Z",
     "start_time": "2019-03-21T17:23:41.794349Z"
    }
   },
   "source": [
    "to_remove = [\"'\", \"´\", \"`\", \",\", \".\", \";\", \"\\\\\", \"+\", \"|\", \")\", \"(\", '\"', \"•\", \"~\", \"{\", \"}\", \"[\", \"]\", \"<\", \">\", \"¤\", \"¨\", \"*\"] \n",
    "to_keep = [\"!\", \"@\", \"#\", \"?\", \"$\", \"€\", \"£\", \"=\"] # keep in list comprehension\n",
    "to_keep_re = [\"&amp\"] # define regex \n",
    "to_sub = [\"-\", \"_\", \"...\", \":\", \"/\", \"\\n\" -> \" \", \"&\" -> \"and\"] # define a regex to substitute (maybe two different re ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T14:42:02.993275Z",
     "start_time": "2019-04-25T14:42:02.991269Z"
    }
   },
   "outputs": [],
   "source": [
    "#### NEED TO FINISH THIS FUNCTION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:24:31.822449Z",
     "start_time": "2019-03-28T14:24:31.812422Z"
    }
   },
   "outputs": [],
   "source": [
    "def tweets_cleaning(tweets, stopwords: list):\n",
    "    cleaned_data = []\n",
    "    for tweet in tweets:\n",
    "        cleaned_tweet = []\n",
    "        tweet = re.sub(r'&amp\\S+','', tweet)\n",
    "        tweet = re.sub(r'&', ' and ', tweet)\n",
    "        tweet = re.sub(r'!!*', '!', tweet)\n",
    "        tweet = re.sub(r'??*', '?', tweet)\n",
    "        tweet = re.sub(r'..*', ' ', tweet)\n",
    "        tweet = re.sub(r'--*', ' ', tweet)\n",
    "        tweet = re.sub(r'__*', ' ', tweet)\n",
    "        tweet = re.sub(r'::*', ' ', tweet)\n",
    "        tweet = re.sub(r'//*', ' ', tweet)\n",
    "        tweet = re.sub(r'\\n', ' ', tweet)\n",
    "        tweet = tweet.split(\" \")\n",
    "        for word in tweet:\n",
    "            # only keep letters (i.e., alphabetical characters) as we want to use word2vec (no digits in word2vec models)\n",
    "            cleaned_word = ''.join([char for char in word if char in to_keep or char.isalpha()]).lower()\n",
    "            # cleaned_word = cleaned_word.strip(\"\\n\") #.strip(\" \") (don't know yet whether latter part is necessary)\n",
    "            if len(cleaned_word) > 0:\n",
    "                cleaned_tweet.append(cleaned_word)\n",
    "        cleaned_data.append(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:20:50.894661Z",
     "start_time": "2019-03-28T16:20:50.887642Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(text_file):\n",
    "\n",
    "    \"\"\" \n",
    "    Read GloVe txt.-file, load pre-trained word embeddings into memory\n",
    "    and create a word_to_embedding dictionary, where keys are the discrete word strings\n",
    "    and values are the corresponding continuous word embeddings, retrieved from the GloVe txt.-file.\n",
    "    For unkown words, the representation is an empty vector (i.e., zeros matrix).\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    with open(text_file, encoding=\"utf8\") as file:\n",
    "\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            wordvec = np.array(values[1:], dtype = 'float32')\n",
    "            embeddings_dict[word] = list(wordvec)\n",
    "\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:27:42.010931Z",
     "start_time": "2019-03-28T14:27:41.661287Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_embeddings = get_embeddings(\"emoji2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:27:57.066502Z",
     "start_time": "2019-03-28T14:27:57.057477Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_emojivecs(emoji_embeddings: dict, corpus: list, dims: int):\n",
    "\n",
    "    N = len(corpus)\n",
    "    M = dims\n",
    "    \n",
    "    emojivecs = []\n",
    "    \n",
    "    # document = tweet; corpus = all tweets\n",
    "    for emoji in corpus:\n",
    "        emoji_sequence = []\n",
    "\n",
    "        try:\n",
    "            emojivec = emoji_embeddings[emoji]\n",
    "            assert len(emojivec) == M\n",
    "            emoji_sequence.append(emojivec)\n",
    "        except KeyError:\n",
    "            emoji_sequence.append([0 for _ in range(M)])\n",
    "            print(\"This {} does not exist in the pre-trained emoji embeddings.\".format(emoji))\n",
    "\n",
    "        emojivecs.append(emoji_sequence)\n",
    "\n",
    "    assert len(emojivecs) == N\n",
    "    return np.array(emojivecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordvecs(word_embeddings: dict, corpus: list, dims: int, zeros_padding = False):\n",
    "\n",
    "    \"\"\" \n",
    "    Return a concatenated word vector representation of each tweet.\n",
    "    The concatenated word vectors serve as the input data for the LSTM RNN.\n",
    "    Each word (embedding) denotes a time step. (Number of timesteps is equal to the length of the input sentence.)\n",
    "    \n",
    "    Check whether length of word vector is equal to the number of dimensions we pass to this function.\n",
    "    For unknown words (i.e., if key does not exist), the representation is an empty vector / zeros matrix of len dims.\n",
    "\n",
    "    Sequences can have variable length (i.e., number of time steps per batch).\n",
    "    However, in some cases you might want to zero pad the batch if a sequence < max length of sequences in the corpus.\n",
    "    By default this argument is set to False as Keras and Tensorflow except input sequences of variable length.\n",
    "    If set to True, zero padding is computed.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(corpus)\n",
    "    M = dims\n",
    "    global max_length\n",
    "    max_length = max([len(sequence) for sequence in corpus])\n",
    "    wordvecs_corpus = []\n",
    "    \n",
    "    # document = tweet; corpus = all tweets\n",
    "    for document in corpus:\n",
    "        wordvec_sequence = []\n",
    "        for word in document:\n",
    "            \n",
    "            try:\n",
    "                wordvec = word_embeddings[word]\n",
    "                assert len(wordvec) == M\n",
    "                wordvec_sequence.append(wordvec)\n",
    "            except KeyError:\n",
    "                wordvec_sequence.append([0 for _ in range(M)])\n",
    "                \n",
    "        # needs to be resolved (!)\n",
    "        if zeros_padding == True: \n",
    "            if len(document) < max_length:\n",
    "\n",
    "                for _ in range(len(document), max_length):\n",
    "                    wordvec_sequence.append([0 for _ in range(M)])\n",
    "\n",
    "                assert len(wordvec_sequence) == max_length\n",
    "        wordvecs_corpus.append(wordvec_sequence)\n",
    "\n",
    "    assert len(wordvecs_corpus) == N\n",
    "    return np.array(wordvecs_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:38:33.022619Z",
     "start_time": "2019-03-28T15:19:41.598193Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model.save_word2vec_format('word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-28T16:20:54.409Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_embeddings = get_embeddings(\"word2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "588.496px",
    "left": "2030px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
