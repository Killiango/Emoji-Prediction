{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed value\n",
    "# to obtain reproducable results set random seed at the beginning of the script\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value = 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as back\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "back.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_RNN():\n",
    "    \n",
    "    def __init__(self, hidden_units: int, n_features: int, n_timesteps, n_labels: int, loss: str, dropout = False, optimizer):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.n_features = n_features\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_labels = n_labels\n",
    "        self.loss = loss\n",
    "        self.dropout = dropout\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # 2 hidden layers, where the second hidden layer has twice as many hidden units than the first\n",
    "        # 1 output layer with softmax as activation function (output = probability distribution over the labels)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(SimpleRNN(hidden_units, return_sequences = True), input_shape = (n_timesteps, n_features))\n",
    "        \n",
    "        if self.dropout != False:\n",
    "            self.model.add(Dropout(dropout))\n",
    "        \n",
    "        self.model.add(RNN(hidden_units * 2, return_sequences = False))\n",
    "        \n",
    "        if self.dropout != False:\n",
    "            self.model.add(Dropout(dropout))\n",
    "        \n",
    "        \n",
    "        self.model.add(Dense(n_labels, activation = 'softmax'))\n",
    "        \n",
    "        self.model.compile(loss = 'categorical_crossentropy', optimizer = self.optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_LSTM():\n",
    "    \n",
    "    def __init__(self, hidden_units: int, n_features: int, n_timesteps, n_labels: int, loss: str, dropout = False, optimizer):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.n_features = n_features\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_labels = n_labels\n",
    "        self.loss = loss\n",
    "        self.dropout = dropout\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # 2 hidden layers, where the second hidden layer has twice as many hidden units than the first\n",
    "        # 1 output layer with softmax as activation function (output = probability distribution over the labels)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(hidden_units, return_sequences = True), input_shape = (n_timesteps, n_features))\n",
    "        \n",
    "        if self.dropout != False:\n",
    "            self.model.add(Dropout(dropout))\n",
    "        \n",
    "        self.model.add(LSTM(hidden_units * 2, return_sequences = False))\n",
    "        \n",
    "        if self.dropout != False:\n",
    "            self.model.add(Dropout(dropout))\n",
    "        \n",
    "        \n",
    "        self.model.add(Dense(n_labels, activation = 'softmax'))\n",
    "        \n",
    "        self.model.compile(loss = 'categorical_crossentropy', optimizer = self.optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bidirectional(hidden_units: int, n_labels: int, n_features: int, n_timesteps = None, mode = 'concat'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return_sequences in the last hidden LSTM layer is required to be set to false, as we want to predict a label\n",
    "    for each document. Output (softmax) layer computes a probability distribution over all k classes for each document.\n",
    "    It predicts the most probable class for each document.\n",
    "    You can change the mode argument (for the bidirectional computation) to any other possible option. \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(hidden_units, return_sequences = True), input_shape = (n_timesteps, n_features), merge_mode = mode))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(hidden_units * 2, return_sequences = False))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(n_labels, activation = 'softmax'))\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important variables to reshape data for LSTM\n",
    "n_samples = Xtrain.shape[0]\n",
    "n_timesteps = Xtrain.shape[1]\n",
    "\n",
    "try:\n",
    "    n_features = Xtrain.shape[2]\n",
    "except:\n",
    "    n_features = dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 20\n",
    "n_labels = K\n",
    "n_batches = 64\n",
    "n_epochs = 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
