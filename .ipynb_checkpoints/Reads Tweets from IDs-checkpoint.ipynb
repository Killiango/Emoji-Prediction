{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T09:23:02.427279Z",
     "start_time": "2019-05-02T09:22:43.465603Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import getopt\n",
    "import logging\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tweepy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:23:11.785556Z",
     "start_time": "2019-03-28T14:23:11.777534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate your own twitter api keys at https://apps.twitter.com/app\n",
    "secret = open('secret.txt')\n",
    "strs = secret.read().split(\"\\n\")\n",
    "CONSUMER_KEY = strs[0]\n",
    "CONSUMER_SECRET = strs[1]\n",
    "OAUTH_TOKEN = strs[2] \n",
    "OAUTH_TOKEN_SECRET = strs[3]\n",
    "\n",
    "# connect to twitter\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# batch size depends on Twitter limit, 100 at this time\n",
    "batch_size = 100\n",
    "\n",
    "#Some emojis have character length of more than 1\n",
    "emoji_threshold = 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:23:17.955554Z",
     "start_time": "2019-03-28T14:23:12.462633Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>imgid</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>742995415264546816</td>\n",
       "      <td>http://pbs.twimg.com/media/Ck80Z5TUYAAb4sM.jpg</td>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>746964081370865664</td>\n",
       "      <td>http://pbs.twimg.com/media/Cl2_3D7WkAAcNYE.jpg</td>\n",
       "      <td>1103,1108,1103,1108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>741083806547857408</td>\n",
       "      <td>http://pbs.twimg.com/media/CH1pK09UYAAhgCh.jpg</td>\n",
       "      <td>1102,1135,1138,1241,1102,1135,1241,1102,1135,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>746749796262645761</td>\n",
       "      <td>http://pbs.twimg.com/media/ClxnlSaUkAAUruc.jpg</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>744903166806786049</td>\n",
       "      <td>http://pbs.twimg.com/media/ClZteXpUgAEzdIW.jpg</td>\n",
       "      <td>820,1105,1413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                           imgid  \\\n",
       "0  742995415264546816  http://pbs.twimg.com/media/Ck80Z5TUYAAb4sM.jpg   \n",
       "1  746964081370865664  http://pbs.twimg.com/media/Cl2_3D7WkAAcNYE.jpg   \n",
       "2  741083806547857408  http://pbs.twimg.com/media/CH1pK09UYAAhgCh.jpg   \n",
       "3  746749796262645761  http://pbs.twimg.com/media/ClxnlSaUkAAUruc.jpg   \n",
       "4  744903166806786049  http://pbs.twimg.com/media/ClZteXpUgAEzdIW.jpg   \n",
       "\n",
       "                                         annotations  \n",
       "0                                                865  \n",
       "1                                1103,1108,1103,1108  \n",
       "2  1102,1135,1138,1241,1102,1135,1241,1102,1135,1...  \n",
       "3                                                186  \n",
       "4                                      820,1105,1413  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'cleaned_img_train_plaintext.txt'\n",
    "#file = 'img_train_plaintext.txt'\n",
    "data = pd.read_csv(file, sep='\\t', encoding = 'utf8', engine='c', header = 0)\n",
    "\n",
    "#data = pd.DataFrame([j for i,j in enumerate(data.values) if len(data.iloc[i, 2].split(',')) == 1], columns=['id','imgid','annotations'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:23:37.982255Z",
     "start_time": "2019-03-28T14:23:37.977241Z"
    }
   },
   "outputs": [],
   "source": [
    "def locate_emoji(emoji_pattern, text: str):\n",
    "    emoji = ''.join(emoji_pattern.findall(text))\n",
    "    try:\n",
    "        index = text.index(emoji)\n",
    "    except:\n",
    "        index = - emoji_threshold\n",
    "    return emoji, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:29:24.897136Z",
     "start_time": "2019-03-28T14:29:24.886108Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tweets(twapi, data):\n",
    "    '''\n",
    "    Fetches content for tweet IDs in a file using bulk request method,\n",
    "    which vastly reduces number of HTTPS requests compared to above;\n",
    "    however, it does not warn about IDs that yield no tweet.\n",
    "    `twapi`: Initialized, authorized API object from Tweepy\n",
    "    '''\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE)\n",
    "\n",
    "    tweet_ids = data.id.values.tolist()\n",
    "    emoji_labels = data.annotations.values.tolist()\n",
    "\n",
    "    all_tweets = []\n",
    "    labels = []\n",
    "    i = 0  #for debug\n",
    "    # process list of ids until it's empty\n",
    "    while len(tweet_ids) > 0:\n",
    "        if len(tweet_ids) < batch_size:\n",
    "            tweets = twapi.statuses_lookup(\n",
    "                id_=tweet_ids, include_entities=False, trim_user=True)\n",
    "            tweet_ids = []\n",
    "        else:\n",
    "            tweets = twapi.statuses_lookup(\n",
    "                id_=tweet_ids[:batch_size],\n",
    "                include_entities=False,\n",
    "                trim_user=True)\n",
    "            tweet_ids = tweet_ids[batch_size:]\n",
    "\n",
    "        for tweet in tweets:\n",
    "            \n",
    "            # removes the link of the tweet\n",
    "            text = re.sub(r'http\\S+', '', tweet.text).strip(' ')            \n",
    "            text = re.sub(r'\\ART @\\S+','', text).strip(' ')\n",
    "            \n",
    "            # remove tweets where emoji is not at the end\n",
    "            emoji, index = locate_emoji(emoji_pattern, text)\n",
    "            \n",
    "            if index >= len(text) - emoji_threshold:\n",
    "                #removes the emojis from the text\n",
    "                text = emoji_pattern.sub(r'', text).strip(' ')\n",
    "\n",
    "                #then appends the tweet and emoji to our final dataset\n",
    "                all_tweets.append(np.array([text]))\n",
    "                labels.append(emoji)\n",
    "\n",
    "        i += 1\n",
    "        if i == 5:\n",
    "            break\n",
    "            \n",
    "    features = all_tweets\n",
    "    #features = np.array(all_tweets)\n",
    "    #labels = np.array(labels)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:29:29.469091Z",
     "start_time": "2019-03-28T14:29:25.994115Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = get_tweets(api, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of stopwords\n",
    "stop_words = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:24:31.822449Z",
     "start_time": "2019-03-28T14:24:31.812422Z"
    }
   },
   "outputs": [],
   "source": [
    "def tweets_cleaning(tweets, stopwords: list):\n",
    "    \"\"\"\n",
    "    Text cleaning function that performs all necessary text preprocessing steps.\n",
    "    Function only keeps characters, that are alphanumerical (non-alphanumerical values are discarded).\n",
    "    Digits are treated by regular expressions.\n",
    "    Lower-casing is performed to reduce noise and normalize the text (convert it into a uniform representation).\n",
    "    Stemming is performed to only keep the stem of each word token but not any other deviated form. \n",
    "    Stop words (i.e., words that occur more frequently than other words in a given corpus) are removed.\n",
    "    \"\"\"\n",
    "    \n",
    "     # initialize Lancaster stemmer\n",
    "    st = LancasterStemmer()\n",
    "    \n",
    "    cleaned_data = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        \n",
    "        cleaned_tweet = []\n",
    "        tweet = re.sub(r'&amp\\S+','', tweet)\n",
    "        tweet = re.sub(r' & ', ' and ', tweet)\n",
    "        tweet = re.sub(r'!!*', '!', tweet)\n",
    "        tweet = re.sub(r'??*', '?', tweet)\n",
    "        tweet = re.sub('[.\\-_:/\\n\\t]+', ' ', tweet)\n",
    "        tweet = tweet.split(\" \")\n",
    "        \n",
    "        for word in tweet:\n",
    "            \n",
    "            # if emoticon is in word, keep the emoticon\n",
    "            if re.search(r'(?:X|:|;|=)(?:-)?(?:\\)|\\(|O|D|P|S)+', word):\n",
    "                cleaned_word = word\n",
    "                \n",
    "            else:\n",
    "                # keep special characters which might carry important information\n",
    "                # perform lower-casing to normalize the text and reduce noise\n",
    "                cleaned_word = ''.join([char for char in word if re.search('[<>$#€£!?@=]', char) or\n",
    "                                        char.isalnum()]).lower()\n",
    "            \n",
    "            if \"<3\" not in cleaned_word:\n",
    "                cleaned_word = re.sub('[0-9]', '0', cleaned_word)\n",
    "  \n",
    "            # removes each \\n (i.e., new line) or \\t (i.e., tab) -> pipe char denotes a disjunction\n",
    "            cleaned_word = re.sub(r'( \\n| \\t)+', '', cleaned_word)\n",
    "            \n",
    "            # perform stemming\n",
    "            cleaned_word = st.stem(cleaned_word)\n",
    "                        \n",
    "            if len(cleaned_word) > 0 and not in stopwords:\n",
    "                cleaned_tweet.append(cleaned_word)\n",
    "            \n",
    "        \n",
    "        if len(cleaned_tweet) > 1:\n",
    "            cleaned_data.append(cleaned_tweet)\n",
    "        \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tweets: list, ngram = (1, 1), vocab = None,):\n",
    "    \"\"\"\n",
    "    Create a count (!) based bag-of-words unigram or bigram representation of provided tweets.\n",
    "    Ngram is set to unigram by default. If bigram bag-of-words should be created, pass tuple (2, 2).\n",
    "    \n",
    "    Vocabulary argument is set to None by default. \n",
    "    You can pass a vocabulary to this function, which may then be used for CountVectorizer. \n",
    "    If you do not pass a vocabulary to this function, CountVectorizer will create a vocabulary itself.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize vectorizer (word-ngram representation)\n",
    "    vectorizer = CountVectorizer(encoding = 'utf-8', lowercase = True, ngram_range = ngram, analyzer = 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:20:50.894661Z",
     "start_time": "2019-03-28T16:20:50.887642Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(text_file):\n",
    "\n",
    "    \"\"\" \n",
    "    Read GloVe txt.-file, load pre-trained word embeddings into memory\n",
    "    and create a word_to_embedding dictionary, where keys are the discrete word strings\n",
    "    and values are the corresponding continuous word embeddings, retrieved from the GloVe txt.-file.\n",
    "    For unkown words, the representation is an empty vector (i.e., zeros matrix).\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    with open(text_file, encoding=\"utf8\") as file:\n",
    "\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            wordvec = np.array(values[1:], dtype = 'float32')\n",
    "            embeddings_dict[word] = list(wordvec)\n",
    "\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:27:42.010931Z",
     "start_time": "2019-03-28T14:27:41.661287Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_embeddings = get_embeddings(\"emoji2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:27:57.066502Z",
     "start_time": "2019-03-28T14:27:57.057477Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_emojivecs(emoji_embeddings: dict, corpus: list, dims: int):\n",
    "\n",
    "    N = len(corpus)\n",
    "    M = dims\n",
    "    \n",
    "    emojivecs = []\n",
    "    \n",
    "    # document = tweet; corpus = all tweets\n",
    "    for emoji in corpus:\n",
    "        emoji_sequence = []\n",
    "\n",
    "        try:\n",
    "            emojivec = emoji_embeddings[emoji]\n",
    "            assert len(emojivec) == M\n",
    "            emoji_sequence.append(emojivec)\n",
    "        except KeyError:\n",
    "            emoji_sequence.append([0 for _ in range(M)])\n",
    "            print(\"This {} does not exist in the pre-trained emoji embeddings.\".format(emoji))\n",
    "\n",
    "        emojivecs.append(emoji_sequence)\n",
    "\n",
    "    assert len(emojivecs) == N\n",
    "    return np.array(emojivecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordvecs(word_embeddings: dict, corpus: list, dims: int, zeros_padding = False):\n",
    "\n",
    "    \"\"\" \n",
    "    Return a concatenated word vector representation of each tweet.\n",
    "    The concatenated word vectors serve as the input data for the LSTM RNN.\n",
    "    Each word (embedding) denotes a time step. (Number of timesteps is equal to the length of the input sentence.)\n",
    "    \n",
    "    Check whether length of word vector is equal to the number of dimensions we pass to this function.\n",
    "    For unknown words (i.e., if key does not exist), the representation is an empty vector / zeros matrix of len dims.\n",
    "\n",
    "    Sequences can have variable length (i.e., number of time steps per batch).\n",
    "    However, in some cases you might want to zero pad the batch if a sequence < max length of sequences in the corpus.\n",
    "    By default this argument is set to False as Keras and Tensorflow except input sequences of variable length.\n",
    "    If set to True, zero padding is computed.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(corpus)\n",
    "    M = dims\n",
    "    global max_length\n",
    "    max_length = max([len(sequence) for sequence in corpus])\n",
    "    wordvecs_corpus = []\n",
    "    \n",
    "    # document = tweet; corpus = all tweets\n",
    "    for document in corpus:\n",
    "        wordvec_sequence = []\n",
    "        for word in document:\n",
    "            \n",
    "            try:\n",
    "                wordvec = word_embeddings[word]\n",
    "                assert len(wordvec) == M\n",
    "                wordvec_sequence.append(wordvec)\n",
    "            except KeyError:\n",
    "                wordvec_sequence.append([0 for _ in range(M)])\n",
    "                \n",
    "        # needs to be resolved (!)\n",
    "        if zeros_padding == True: \n",
    "            if len(document) < max_length:\n",
    "\n",
    "                for _ in range(len(document), max_length):\n",
    "                    wordvec_sequence.append([0 for _ in range(M)])\n",
    "\n",
    "                assert len(wordvec_sequence) == max_length\n",
    "        wordvecs_corpus.append(wordvec_sequence)\n",
    "\n",
    "    assert len(wordvecs_corpus) == N\n",
    "    return np.array(wordvecs_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:38:33.022619Z",
     "start_time": "2019-03-28T15:19:41.598193Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model.save_word2vec_format('word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-28T16:20:54.409Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_embeddings = get_embeddings(\"word2vec.txt\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "588.496px",
    "left": "2030px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
