{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:43:27.059352Z",
     "start_time": "2019-05-22T10:43:27.045317Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "import getopt\n",
    "import logging\n",
    "#import nltk\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tweepy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, SimpleRNN\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "#from RNNs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:33:27.372470Z",
     "start_time": "2019-05-22T10:33:26.541259Z"
    }
   },
   "outputs": [],
   "source": [
    "train_proc = pd.read_csv('train_set_processed.csv')\n",
    "val_proc = pd.read_csv('val_set_processed.csv')\n",
    "test_proc = pd.read_csv('test_set_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:33:28.068322Z",
     "start_time": "2019-05-22T10:33:28.058295Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def count_emojis(data, n = 10):\n",
    "    \"\"\"\n",
    "    Function that counts the number of emojis in the data set.\n",
    "    Display the n most frequent emojis.\n",
    "    \"\"\"\n",
    "    emoji_counts = {}\n",
    "    for index, row in data.iterrows():\n",
    "        emoji = row[1]\n",
    "        if emoji not in emoji_counts:\n",
    "            # compute simultaneous counting\n",
    "            emoji_counts[emoji] = data[data.label == emoji].count()[1]\n",
    "            \n",
    "    # sort emojis by freq in descending order (list of tuples will be returned)\n",
    "    sorted_emoji_counts = sorted(emoji_counts.items(), key= lambda kv: kv[1], reverse=True)\n",
    "        \n",
    "    return [emoji[0] for emoji in sorted_emoji_counts[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:33:37.745062Z",
     "start_time": "2019-05-22T10:33:28.649868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['üòç', 'üòÇ', '‚ù§Ô∏è', 'üíï', 'üòä', 'üòò', 'üò≠', 'üíñ', 'üòé', '‚ú®']\n"
     ]
    }
   ],
   "source": [
    "top_10_test = count_emojis(test_proc)\n",
    "print(top_10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:33:38.372732Z",
     "start_time": "2019-05-22T10:33:38.367719Z"
    }
   },
   "outputs": [],
   "source": [
    "### emoji map for top 10 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:33:39.031484Z",
     "start_time": "2019-05-22T10:33:39.026471Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_map = {emoji: i for i, emoji in enumerate(top_10_test)}\n",
    "idx_emoji = {i: emoji for i, emoji in enumerate(top_10_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:52.905839Z",
     "start_time": "2019-05-18T11:50:52.901828Z"
    }
   },
   "outputs": [],
   "source": [
    "distinct_8 = [emoji for emoji, i in emoji_map.items() if i is not 3 and i is not 7]\n",
    "print(distinct_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:53.601574Z",
     "start_time": "2019-05-18T11:50:53.598567Z"
    }
   },
   "outputs": [],
   "source": [
    "### emoji map for distinct 8 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:54.106919Z",
     "start_time": "2019-05-18T11:50:54.101904Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_map = {emoji: i for i, emoji in enumerate(distinct_8)}\n",
    "idx_emoji = {i: emoji for i, emoji in enumerate(distinct_8)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:33:39.542844Z",
     "start_time": "2019-05-22T10:33:39.537830Z"
    }
   },
   "outputs": [],
   "source": [
    "def emoji_to_int(labels: list):\n",
    "    return [emoji_map[emoji] for emoji in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:56.347514Z",
     "start_time": "2019-05-18T11:50:56.340494Z"
    }
   },
   "outputs": [],
   "source": [
    "def distinct_eight(data, distinct_8: list): \n",
    "    \"\"\"\n",
    "    Function that checks, whether Tweet consists of one of the distinct eight emojis.\n",
    "    The distinct eight emojis are a subset of the ten most frequent emojis in the dataset,\n",
    "    where emojis with similar meaning were filtered.\n",
    "    If, and only if, Tweet consists one of those emojis, \n",
    "    Tweet will be used for further analysis.\n",
    "    Else: Line will be dropped.\n",
    "    \"\"\"\n",
    "    idx_drop = []\n",
    "    for index, row in data.iterrows():\n",
    "        if row[1] not in distinct_8:\n",
    "            idx_drop.append(index)\n",
    "    return data.drop(data.index[idx_drop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:33:58.092972Z",
     "start_time": "2019-05-22T10:33:58.087958Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def keep_top_10(data, top_10: list): \n",
    "    \"\"\"\n",
    "    Function that checks, whether Tweet consists of one of the top ten emojis.\n",
    "    If, and only if, Tweet consists one of the most frequent emojis, \n",
    "    Tweet will be used for further analysis.\n",
    "    Else: Line will be dropped.\n",
    "    \"\"\"\n",
    "    idx_drop = []\n",
    "    for index, row in data.iterrows():\n",
    "        if row[1] not in top_10:\n",
    "            idx_drop.append(index)\n",
    "    return data.drop(data.index[idx_drop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T08:34:24.363544Z",
     "start_time": "2019-05-18T08:34:24.360548Z"
    }
   },
   "outputs": [],
   "source": [
    "### distinct 8 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:51:46.219497Z",
     "start_time": "2019-05-18T11:50:59.209113Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = distinct_eight(train_proc, distinct_8)\n",
    "print(\"Number of Tweets in the train data set: {}\".format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:51:51.487498Z",
     "start_time": "2019-05-18T11:51:46.381929Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = distinct_eight(test_proc, distinct_8)\n",
    "print(\"Number of Tweets in the test data set: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:51:56.156109Z",
     "start_time": "2019-05-18T11:51:51.689033Z"
    }
   },
   "outputs": [],
   "source": [
    "val_data = distinct_eight(val_proc, distinct_8)\n",
    "print(\"Number of Tweets in the validation data set: {}\".format(len(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### top 10 ### (don't execute for now, maybe useful for comparison with distinct eight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:34:40.855947Z",
     "start_time": "2019-05-22T10:34:03.814369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets in the train data set: 81236\n"
     ]
    }
   ],
   "source": [
    "train_data = keep_top_10(train_proc, top_10_test)\n",
    "print(\"Number of Tweets in the train data set: {}\".format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:34:44.841549Z",
     "start_time": "2019-05-22T10:34:41.449528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets in the test data set: 7646\n"
     ]
    }
   ],
   "source": [
    "test_data = keep_top_10(test_proc, top_10_test)\n",
    "print(\"Number of Tweets in the test data set: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:34:48.959503Z",
     "start_time": "2019-05-22T10:34:45.623631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets in the validation data set: 7613\n"
     ]
    }
   ],
   "source": [
    "val_data = keep_top_10(val_proc, top_10_test)\n",
    "print(\"Number of Tweets in the validation data set: {}\".format(len(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:34:49.785702Z",
     "start_time": "2019-05-22T10:34:49.768656Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create list of stopwords\n",
    "stop_words = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:34:50.659025Z",
     "start_time": "2019-05-22T10:34:50.620924Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tweets_cleaning(tweets, labels, stopwords: list, train = False, use_bigrams = False, \n",
    "                    lowercase = True, stemming = False, min_df = 2, embedding = False):\n",
    "    \"\"\"\n",
    "    Text cleaning function that performs all necessary text preprocessing steps.\n",
    "    Function only keeps characters, that are alphanumerical (non-alphanumerical values are discarded).\n",
    "    Digits are treated by regular expressions.\n",
    "    Lower-casing is performed to reduce noise and normalize the text (convert it into a uniform representation).\n",
    "    Stemming is performed to only keep the stem of each word token but not any other deviated form. \n",
    "    Stop words (i.e., words that occur more frequently than other words in a given corpus) are removed.\n",
    "    \"\"\"\n",
    "    \n",
    "     # initialize Lancaster stemmer\n",
    "    if stemming:\n",
    "        st = LancasterStemmer()\n",
    "    \n",
    "    cleaned_data = []\n",
    "    cleaned_labels = []\n",
    "    \n",
    "    all_bigrams = [] # serves as place-holder\n",
    "    bigrams_dict = dict()\n",
    "    vocab = dict()\n",
    "    \n",
    "    for tweet, label in zip(tweets, labels):\n",
    "        tweet = re.sub(r'&amp\\S+','', tweet)\n",
    "        tweet = re.sub(r' & ', ' and ', tweet)\n",
    "        tweet = re.sub(r'!+', ' ! ', tweet)\n",
    "        tweet = re.sub(r'[?]+', ' ? ', tweet)\n",
    "        tweet = re.sub('@.+', '@user', tweet)\n",
    "        tweet = re.sub('#', '# ', tweet)\n",
    "\n",
    "        # Create spaces instead of some punctuation marks, but not if it's part of an emoticon\n",
    "        tweet = ' '.join([word if re.search(r'(?:X|:|;|=)(?:-)?(?:\\)|\\(|O|D|P|S)+', word)\n",
    "            else re.sub('[,.;\\-_:/\\n\\t]+', ' ', word) for word in tweet.split()])\n",
    "        \n",
    "        tweet = tweet.split(\" \")\n",
    "        \n",
    "        cleaned_tweet = []\n",
    "        for word in tweet:\n",
    "            \n",
    "            #if emoticon is in word, keep the emoticon\n",
    "            if re.search(r'(?:X|:|;|=)(?:-)?(?:\\)|\\(|O|D|P|S)+', word):\n",
    "                cleaned_word = word\n",
    "            else:\n",
    "                # keep special characters which might carry important information\n",
    "                # perform lower-casing to normalize the text and reduce noise\n",
    "                cleaned_word = ''.join([char for char in word if re.search('[<>$#‚Ç¨¬£!?@=]', char) or\n",
    "                                        char.isalnum()])\n",
    "            if lowercase:\n",
    "                cleaned_word = cleaned_word.lower()\n",
    "                \n",
    "            if \"<3\" not in cleaned_word:\n",
    "                cleaned_word = re.sub('[0-9]', '0', cleaned_word)\n",
    "  \n",
    "            # removes each \\n (i.e., new line) or \\t (i.e., tab) -> pipe char denotes a disjunction\n",
    "            cleaned_word = re.sub(r'( \\n| \\t)+', '', cleaned_word)\n",
    "            \n",
    "            if stemming:\n",
    "                cleaned_word = st.stem(cleaned_word)\n",
    "            \n",
    "            if len(cleaned_word) > 0 and cleaned_word not in stopwords:\n",
    "                cleaned_tweet.append(cleaned_word)\n",
    "                \n",
    "                if train:\n",
    "                    if cleaned_word in vocab:\n",
    "                        vocab[cleaned_word] += 1\n",
    "                    else:\n",
    "                        vocab[cleaned_word] = 1\n",
    "            \n",
    "        # only append tweets with more than 1 word per tweet\n",
    "        if len(cleaned_tweet) > 1:\n",
    "            \n",
    "            if train and use_bigrams:\n",
    "                \n",
    "                bigrams = [' '.join([cleaned_tweet[i-1], cleaned_tweet[i]]) \n",
    "                           for i, _ in enumerate(cleaned_tweet) if i > 0]\n",
    "                \n",
    "                for bigram in bigrams:\n",
    "                    \n",
    "                    if bigram in bigrams_dict:\n",
    "                        bigrams_dict[bigram] += 1\n",
    "                    else:\n",
    "                        bigrams_dict[bigram] = 1 \n",
    "\n",
    "            cleaned_tweet = ' '.join(cleaned_tweet)\n",
    "            cleaned_data.append(cleaned_tweet)\n",
    "            cleaned_labels.append(label)\n",
    "            \n",
    "    if train and embedding and not use_bigrams:\n",
    "        \n",
    "        word2index = dict()\n",
    "        i = 1\n",
    "        for word in vocab.keys():\n",
    "            word2index[word] = i\n",
    "            i += 1\n",
    "            \n",
    "        word2index.update({'UNK': len(word2idx) + 1})\n",
    "        \n",
    "        assert len(cleaned_data) == len(cleaned_labels)\n",
    "\n",
    "        return cleaned_data, cleaned_labels, word2index\n",
    "                \n",
    "    if train:\n",
    "        vocab = [word for word, freq in vocab.items() if freq >= min_df]\n",
    "            \n",
    "        \n",
    "        if use_bigrams:\n",
    "            all_bigrams = [bigram for bigram, freq in bigrams_dict.items() if freq >= min_df]\n",
    "            vocab.extend(all_bigrams)\n",
    "        \n",
    "        #cleaned_data = [tweet.split() for tweet in cleaned_data]\n",
    "        \n",
    "        #for i, tweet in enumerate(cleaned_data):\n",
    "        #    for j, word in enumerate(tweet):\n",
    "        #        \n",
    "        #        if word not in vocab:\n",
    "        #            cleaned_data[i].pop(j)\n",
    "        #            \n",
    "        #    if len(cleaned_data) < 2:\n",
    "        #        cleaned_data.pop(i)\n",
    "        #        cleaned_labels.pop(i)\n",
    "        #        \n",
    "        #cleaned_data = [' '.join(tweet) for tweet in cleaned_data]\n",
    "        \n",
    "    assert len(cleaned_data) == len(cleaned_labels)\n",
    "    \n",
    "    return cleaned_data, cleaned_labels, sorted(vocab), sorted(all_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:35:31.273059Z",
     "start_time": "2019-05-22T10:34:51.443111Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_train_data, train_labels, vocab, bigrams = tweets_cleaning(train_data.text, \n",
    "                                                                   train_data.label, \n",
    "                                                                   stop_words, \n",
    "                                                                   train = True, \n",
    "                                                                   use_bigrams = True, \n",
    "                                                                   lowercase = True,\n",
    "                                                                   min_df = 4)\n",
    "\n",
    "cleaned_test_data, test_labels, _, _ = tweets_cleaning(test_data.text, \n",
    "                                                       test_data.label, \n",
    "                                                       stop_words, \n",
    "                                                       lowercase = True)\n",
    "\n",
    "cleaned_val_data, val_labels, _, _ = tweets_cleaning(val_data.text, \n",
    "                                                     val_data.label, \n",
    "                                                     stop_words, \n",
    "                                                     lowercase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:35:32.732943Z",
     "start_time": "2019-05-22T10:35:32.721913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets per data set after text cleaning was computed:\n",
      "\n",
      "Train: 64540\n",
      "\n",
      "Test: 6142\n",
      "\n",
      "Validation: 6114\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Tweets per data set after text cleaning was computed:\")\n",
    "print()\n",
    "print(\"Train: {}\".format(len(cleaned_train_data)))\n",
    "print()\n",
    "print(\"Test: {}\".format(len(cleaned_test_data)))\n",
    "print()\n",
    "print(\"Validation: {}\".format(len(cleaned_val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:35:34.325178Z",
     "start_time": "2019-05-22T10:35:34.321168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the vocabulary: 14699\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique tokens in the vocabulary: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:35:36.050767Z",
     "start_time": "2019-05-22T10:35:36.034725Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = emoji_to_int(train_labels)\n",
    "y_test = emoji_to_int(test_labels)\n",
    "y_val = emoji_to_int(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for the Bag of Words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:35:37.688124Z",
     "start_time": "2019-05-22T10:35:37.678098Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bag_of_words(train: list, test: list, val: list, ngram: tuple, vocab = None, \n",
    "                 n_best_factor = 0.7):\n",
    "    \"\"\"\n",
    "    Create a weighted bag-of-words unigram or bigram representation of provided tweets.\n",
    "    Ngram is set to unigram by default. If bigram bag-of-words should be created, pass tuple (2, 2).\n",
    "    \n",
    "    Vocabulary argument is set to None by default. \n",
    "    You can pass a vocabulary to this function, which may then be used for TfidfVectorizer. \n",
    "    If you do not pass a vocabulary to this function, TfidfVectorizer will create a vocabulary itself.\n",
    "    \"\"\" \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(encoding = 'utf-8', ngram_range = ngram, analyzer = 'word', \n",
    "                                 vocabulary = vocab, min_df = 0.1, max_df = 0.9, norm = 'l2',\n",
    "                                 smooth_idf = True, sublinear_tf = True)\n",
    "    \n",
    "    train_BoW = vectorizer.fit_transform(train).toarray()\n",
    "    test_BoW = vectorizer.transform(test).toarray()\n",
    "    val_BoW = vectorizer.transform(val).toarray()\n",
    "    \n",
    "    #n_best = int(len(vectorizer.idf_) * n_best_factor)\n",
    "    #idx = np.argsort(vectorizer.idf_)[:n_best]\n",
    "\n",
    "    #train_BoW = train_BoW[:, idx]\n",
    "    #test_BoW = test_BoW[:, idx]\n",
    "    #val_BoW = val_BoW[:, idx]\n",
    "\n",
    "\n",
    "    return train_BoW, test_BoW, val_BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:35:39.317457Z",
     "start_time": "2019-05-22T10:35:39.310440Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_cat_matrix(y):\n",
    "\n",
    "    \"\"\" \n",
    "    Binary one-hot encoding using an indicator matrix.\n",
    "    This function converts labels to a categorical matrix which is of size N x K.\n",
    "    Each row is a row vector with k-1 zeros and a single 1.\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    K = len(set(y))\n",
    "    ind_matrix = np.zeros((N,K), dtype = int)\n",
    "    \n",
    "    for i, cat in enumerate(y):\n",
    "        ind_matrix[i, int(cat)] = 1\n",
    "        \n",
    "    return ind_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:35:43.223849Z",
     "start_time": "2019-05-22T10:35:40.993916Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, X_val = bag_of_words(cleaned_train_data, cleaned_test_data, cleaned_val_data, ngram = (1, 2), vocab = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:50:22.005913Z",
     "start_time": "2019-05-22T09:50:22.002906Z"
    }
   },
   "outputs": [],
   "source": [
    "#np.savetxt('X_train_BoW.txt', X_train)\n",
    "#np.savetxt('X_test_BoW.txt', X_test)\n",
    "#np.savetxt('X_val_BoW.txt', X_val)\n",
    "\n",
    "#np.savetxt('y_train_BoW.txt', y_train)\n",
    "#np.savetxt('y_test_BoW.txt', y_test)\n",
    "#np.savetxt('y_val_BoW.txt', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network (Multilayer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:35:52.568950Z",
     "start_time": "2019-05-22T10:35:52.559926Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(hidden_units: int, input_dims: int, n_labels: int):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, input_dim = input_dims, activation = 'relu'))\n",
    "    model.add(Dropout(0.5)) # dropout is important to prevent model from overfitting\n",
    "    model.add(Dense(n_labels, activation = 'softmax'))\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1 = 0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:35:54.128097Z",
     "start_time": "2019-05-22T10:35:54.122081Z"
    }
   },
   "outputs": [],
   "source": [
    "def preds_to_labels(ypred):\n",
    "    \"\"\"\n",
    "    Firstly, extract the predicted label from a vector of probability distributions.\n",
    "    Secondly, retrieve index of highest value (i.e., highest probability).\n",
    "    \"\"\"\n",
    "    num_labels = [np.argmax(pred) for pred in ypred]\n",
    "    return np.array(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:36:03.705122Z",
     "start_time": "2019-05-22T10:36:03.701112Z"
    }
   },
   "outputs": [],
   "source": [
    "# set number of hidden units, epochs and batch size\n",
    "n_units = 50\n",
    "n_epochs = 6\n",
    "n_batches = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:45:51.502545Z",
     "start_time": "2019-05-22T10:45:34.253804Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b99899ee3dae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmax_abs_scaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaxAbsScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_abs_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_abs_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_abs_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_abs_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    915\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[0;32m    916\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmay_share_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m     if (warn_on_dtype and dtypes_orig is not None and\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#max_abs_scaler = MaxAbsScaler()\n",
    "#X_train = max_abs_scaler.fit_transform(X_train)\n",
    "#X_val = max_abs_scaler.transform(X_val)\n",
    "#X_test = max_abs_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:50:52.887006Z",
     "start_time": "2019-05-22T09:50:26.905897Z"
    }
   },
   "outputs": [],
   "source": [
    "# shuffle data before fitting the neural network with it\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_val, y_val = shuffle(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:36:06.782967Z",
     "start_time": "2019-05-22T10:36:06.749879Z"
    }
   },
   "outputs": [],
   "source": [
    "# get indicator matrix with one-hot-encoded vectors per label (of all labels)\n",
    "y_train = to_cat_matrix(y_train)\n",
    "y_val = to_cat_matrix(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:36:10.346834Z",
     "start_time": "2019-05-22T10:36:08.322061Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_model(n_units, X_train.shape[1], y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:36:11.755583Z",
     "start_time": "2019-05-22T10:36:11.750569Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:43:08.014155Z",
     "start_time": "2019-05-22T10:36:13.241536Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64540 samples, validate on 6114 samples\n",
      "Epoch 1/6\n",
      "64540/64540 [==============================] - 72s 1ms/step - loss: 2.0133 - acc: 0.3122 - val_loss: 1.8608 - val_acc: 0.3742\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.37422, saving model to best_model.h5\n",
      "Epoch 2/6\n",
      "64540/64540 [==============================] - 77s 1ms/step - loss: 1.8109 - acc: 0.3892 - val_loss: 1.7865 - val_acc: 0.3889\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.37422 to 0.38894, saving model to best_model.h5\n",
      "Epoch 3/6\n",
      "64540/64540 [==============================] - 79s 1ms/step - loss: 1.6923 - acc: 0.4275 - val_loss: 1.7596 - val_acc: 0.3955\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.38894 to 0.39549, saving model to best_model.h5\n",
      "Epoch 4/6\n",
      "64540/64540 [==============================] - 75s 1ms/step - loss: 1.6062 - acc: 0.4539 - val_loss: 1.7547 - val_acc: 0.4033\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.39549 to 0.40334, saving model to best_model.h5\n",
      "Epoch 5/6\n",
      "64540/64540 [==============================] - 75s 1ms/step - loss: 1.5319 - acc: 0.4779 - val_loss: 1.7619 - val_acc: 0.4042\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.40334 to 0.40415, saving model to best_model.h5\n",
      "Epoch 6/6\n",
      "31136/64540 [=============>................] - ETA: 37s - loss: 1.4550 - acc: 0.5003"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d1fbbf48d273>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = n_epochs, \n\u001b[1;32m----> 2\u001b[1;33m           batch_size = n_batches, callbacks = [es, mc])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = n_epochs, \n",
    "          batch_size = n_batches, callbacks = [es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:59:40.532295Z",
     "start_time": "2019-05-22T09:59:38.640262Z"
    }
   },
   "outputs": [],
   "source": [
    "# load best model\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:59:44.351455Z",
     "start_time": "2019-05-22T09:59:42.238836Z"
    }
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_pred_test = saved_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:59:46.302645Z",
     "start_time": "2019-05-22T09:59:46.269558Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert predictions to labels\n",
    "y_pred_labels = preds_to_labels(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:58:19.845584Z",
     "start_time": "2019-05-22T09:58:19.830544Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_top_n(y_true, y_preds, top_n = 3):\n",
    "    \"\"\"\n",
    "    If the correct label / emoji is among the top n (e.g., two, three) predictions,\n",
    "    we consider the prediction as correctly labeled.\n",
    "    \"\"\"\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    \n",
    "    for i, pred in enumerate(y_preds):\n",
    "        top_3 = np.argsort(pred)[-top_n:]\n",
    "        if y_true[i] in top_3:\n",
    "            n_correct += 1\n",
    "        n_total += 1\n",
    "        \n",
    "    ratio = n_correct / n_total\n",
    "    return round(ratio, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:58:21.735612Z",
     "start_time": "2019-05-22T09:58:21.609276Z"
    }
   },
   "outputs": [],
   "source": [
    "# if true label is among the top 3 predictions, prediction is deemed correctly labeled\n",
    "accuracy_top_n(y_test, y_pred_test, top_n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:58:24.025030Z",
     "start_time": "2019-05-22T09:58:23.899695Z"
    }
   },
   "outputs": [],
   "source": [
    "# if true label is among the top 2 predictions, prediction is deemed correctly labeled\n",
    "accuracy_top_n(y_test, y_pred_test, top_n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:58:25.870940Z",
     "start_time": "2019-05-22T09:58:25.850886Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:58:27.636636Z",
     "start_time": "2019-05-22T09:58:27.618588Z"
    }
   },
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_labels, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:58:29.303069Z",
     "start_time": "2019-05-22T09:58:29.291038Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_labels, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T09:58:31.064756Z",
     "start_time": "2019-05-22T09:58:31.036681Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_labels, target_names=top_10_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:55:03.628619Z",
     "start_time": "2019-05-18T13:55:03.616588Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for tweet, pred, true in zip(cleaned_test_data, y_pred_labels, test_labels):\n",
    "    print(tweet)\n",
    "    print(\"prediction:\", idx_emoji[pred])\n",
    "    print(\"true label:\", true)\n",
    "    print()\n",
    "    if i == 30:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:54:05.379606Z",
     "start_time": "2019-05-18T13:54:05.368577Z"
    }
   },
   "outputs": [],
   "source": [
    "freq = {}\n",
    "for pred in y_pred_labels:\n",
    "    if idx_emoji[pred] in freq:\n",
    "        freq[idx_emoji[pred]] += 1\n",
    "    else:\n",
    "        freq[idx_emoji[pred]] = 1 \n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:54:06.444436Z",
     "start_time": "2019-05-18T13:54:06.436417Z"
    }
   },
   "outputs": [],
   "source": [
    "freq = {}\n",
    "for y_true in y_test:\n",
    "    if idx_emoji[y_true] in freq:\n",
    "        freq[idx_emoji[y_true]] += 1\n",
    "    else:\n",
    "        freq[idx_emoji[y_true]] = 1 \n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT PART ONLY FOR RESEARCH PAPER BUT NOT FOR COGSCI II PROJECT !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:54:48.328807Z",
     "start_time": "2019-05-17T11:54:33.275750Z"
    }
   },
   "outputs": [],
   "source": [
    "lower = True\n",
    "\n",
    "cleaned_train_data, train_labels, word2idx = tweets_cleaning(train_data.text, \n",
    "                                                                   train_data.label, \n",
    "                                                                   stop_words, \n",
    "                                                                   train = True, \n",
    "                                                                   use_bigrams = False, \n",
    "                                                                   lowercase = lower,\n",
    "                                                                   min_df = 2,\n",
    "                                                                   embedding = True)\n",
    "\n",
    "cleaned_test_data, test_labels, _, _ = tweets_cleaning(test_data.text, \n",
    "                                                       test_data.label, \n",
    "                                                       stop_words, \n",
    "                                                       lowercase = lower)\n",
    "\n",
    "cleaned_val_data, val_labels, _, _ = tweets_cleaning(val_data.text, \n",
    "                                                     val_data.label, \n",
    "                                                     stop_words, \n",
    "                                                     lowercase = lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T14:37:27.826469Z",
     "start_time": "2019-05-17T14:37:27.743248Z"
    }
   },
   "outputs": [],
   "source": [
    "# only convert y_train and y_val to categorical matrix\n",
    "y_train = to_cat_matrix(emoji_to_int(train_labels))\n",
    "y_val = to_cat_matrix(emoji_to_int(val_labels))\n",
    "\n",
    "y_test = emoji_to_int(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:56:24.593461Z",
     "start_time": "2019-05-17T11:56:24.584438Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent2idx(word2idx: dict, documents: list):\n",
    "    \n",
    "    idx_docs = list()\n",
    "    max_length = max([len(document) for document in documents])\n",
    "    \n",
    "    for document in documents: \n",
    "        idx_doc = [word2idx[word] if word in word2idx else word2idx['UNK'] \n",
    "                   for word in document.split()]\n",
    "        \n",
    "        if len(idx_doc) < max_length:\n",
    "            idx_doc.extend([0 for _ in range(max_length - len(idx_doc))])\n",
    "            \n",
    "        idx_docs.append(idx_doc)\n",
    "        \n",
    "    return np.array(idx_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:01.794452Z",
     "start_time": "2019-05-17T11:57:00.331560Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = sent2idx(word2idx, cleaned_train_data)\n",
    "X_val = sent2idx(word2idx, cleaned_val_data)\n",
    "X_test = sent2idx(word2idx, cleaned_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data before fitting the neural network with it\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_val, y_val = shuffle(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T12:10:54.221067Z",
     "start_time": "2019-05-17T12:10:54.212043Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_embeddings(text_file, dim):\n",
    "\n",
    "    \"\"\" \n",
    "    Read GloVe txt.-file, load pre-trained word embeddings into memory\n",
    "    and create a word_to_embedding dictionary, where keys are the discrete word strings\n",
    "    and values are the corresponding continuous word embeddings, retrieved from the GloVe txt.-file.\n",
    "    For unkown words, the representation is an empty vector (i.e., zeros matrix).\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    with open(text_file, encoding=\"utf8\") as file:\n",
    "\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            wordvec = np.array(values[1:], dtype = 'float32')\n",
    "            embeddings_dict[word] = list(wordvec)\n",
    "    \n",
    "    embeddings_dict.update({'UNK': [0 for _ in range(dim)]})\n",
    "\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:27:42.010931Z",
     "start_time": "2019-03-28T14:27:41.661287Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_embeddings = get_embeddings(\"emoji2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:27:57.066502Z",
     "start_time": "2019-03-28T14:27:57.057477Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_emojivecs(emoji_embeddings: dict, corpus: list, dims: int):\n",
    "\n",
    "    N = len(corpus)\n",
    "    M = dims\n",
    "    \n",
    "    emojivecs = []\n",
    "    \n",
    "    # document = tweet; corpus = all tweets\n",
    "    for emoji in corpus:\n",
    "        emoji_sequence = []\n",
    "\n",
    "        try:\n",
    "            emojivec = emoji_embeddings[emoji]\n",
    "            assert len(emojivec) == M\n",
    "            emoji_sequence.append(emojivec)\n",
    "        except KeyError:\n",
    "            emoji_sequence.append([0 for _ in range(M)])\n",
    "            print(\"This {} does not exist in the pre-trained emoji embeddings.\".format(emoji))\n",
    "\n",
    "        emojivecs.append(emoji_sequence)\n",
    "\n",
    "    assert len(emojivecs) == N\n",
    "    return np.array(emojivecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_wordvecs(word_embeddings: dict, corpus: list, dims: int, zeros_padding = False):\n",
    "\n",
    "    \"\"\" \n",
    "    Return a concatenated word vector representation of each tweet.\n",
    "    The concatenated word vectors serve as the input data for the LSTM RNN.\n",
    "    Each word (embedding) denotes a time step. (Number of timesteps is equal to the length of the input sentence.)\n",
    "    \n",
    "    Check whether length of word vector is equal to the number of dimensions we pass to this function.\n",
    "    For unknown words (i.e., if key does not exist), the representation is an empty vector / zeros matrix of len dims.\n",
    "\n",
    "    Sequences can have variable length (i.e., number of time steps per batch).\n",
    "    However, in some cases you might want to zero pad the batch if a sequence < max length of sequences in the corpus.\n",
    "    By default this argument is set to False as Keras and Tensorflow except input sequences of variable length.\n",
    "    If set to True, zero padding is computed.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(corpus)\n",
    "    M = dims\n",
    "    global max_length\n",
    "    max_length = max([len(sequence) for sequence in corpus])\n",
    "    wordvecs_corpus = []\n",
    "    \n",
    "    # document = tweet; corpus = all tweets\n",
    "    for document in corpus:\n",
    "        wordvec_sequence = []\n",
    "        for word in document:\n",
    "            \n",
    "            try:\n",
    "                wordvec = word_embeddings[word]\n",
    "                assert len(wordvec) == M\n",
    "                wordvec_sequence.append(wordvec)\n",
    "            except KeyError:\n",
    "                wordvec_sequence.append([0 for _ in range(M)])\n",
    "                \n",
    "        # needs to be resolved (!)\n",
    "        if zeros_padding == True: \n",
    "            if len(document) < max_length:\n",
    "\n",
    "                for _ in range(len(document), max_length):\n",
    "                    wordvec_sequence.append([0 for _ in range(M)])\n",
    "\n",
    "                assert len(wordvec_sequence) == max_length\n",
    "        wordvecs_corpus.append(wordvec_sequence)\n",
    "\n",
    "    assert len(wordvecs_corpus) == N\n",
    "    return np.array(wordvecs_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T12:14:13.632312Z",
     "start_time": "2019-05-17T12:14:13.622288Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding_matrix(word2idx: dict, embeddings_dict: dict, dim: int):\n",
    "    \n",
    "    embedding_mat = np.zeros((len(word2idx) + 2, dim))\n",
    "    \n",
    "    for word, idx in word2idx.items():\n",
    "        vec = embeddings_dict.get(word)\n",
    "        # if word is not found in embeddings dictionary, vector will be all zeros\n",
    "        if vec is not None:\n",
    "            embedding_mat[idx] = vec\n",
    "            \n",
    "    return embedding_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T12:11:48.526604Z",
     "start_time": "2019-05-17T12:11:29.939513Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_embeddings = get_embeddings(\"glove.6B.50d.txt\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T12:14:49.561920Z",
     "start_time": "2019-05-17T12:14:49.348352Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_mat = embedding_matrix(word2idx, word_embeddings, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:35:18.790718Z",
     "start_time": "2019-05-17T16:35:18.751614Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)\n",
    "hidden_units = 50\n",
    "n_features = 50\n",
    "n_labels = 10\n",
    "optimizer = keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, \n",
    "                                  decay = 0.0, amsgrad = False)\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:36:19.662697Z",
     "start_time": "2019-05-17T16:36:19.634621Z"
    }
   },
   "outputs": [],
   "source": [
    "class GRU_NET():\n",
    "\n",
    "    #embedding_dim = 300\n",
    "    \n",
    "    def __init__(self, vocab_size: int, hidden_units: int, n_features: int, embedding_matrix, \n",
    "                 n_labels: int, optimizer, dropout = 0.1):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.n_features = n_features\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.n_labels = n_labels\n",
    "        # if we want to predict emoji vecs instead of emoji labels, use cosine proximity\n",
    "        self.loss = \"categorical_crossentropy\" \n",
    "        self.optimizer = optimizer\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        print('Build model...')\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(Embedding(vocab_size + 2, n_features, weights = [embedding_matrix], \n",
    "                                 trainable = False, mask_zero = True))\n",
    "        \n",
    "        self.model.add(GRU(hidden_units, activation='relu', recurrent_activation='hard_sigmoid', \n",
    "                           return_sequences = True))    \n",
    "        \n",
    "        self.model.add(Dropout(dropout))\n",
    "        \n",
    "        self.model.add(GRU(hidden_units, activation='relu', recurrent_activation='hard_sigmoid', \n",
    "                           return_sequences = False))\n",
    "        \n",
    "        self.model.add(Dropout(dropout))\n",
    "        \n",
    "        #self.model.add(TimeDistributed(Dense(self.n_labels, activation = 'softmax')))\n",
    "        self.model.add(Dense(self.n_labels, activation = 'softmax'))\n",
    "        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = ['accuracy'])\n",
    "                       \n",
    "    def fit(self, X_train, y_train, X_val, y_val,  n_epochs, n_batches):\n",
    "        return self.model.fit(X_train, y_train, validation_data = (X_val, y_val), \n",
    "                              epochs = n_epochs, batch_size = n_batches)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:01:33.060710Z",
     "start_time": "2019-05-17T18:01:33.043663Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_NET():\n",
    "\n",
    "    #embedding_dim = 300\n",
    "\n",
    "    def __init__(self, vocab_size: int, hidden_units: int, n_features: int, embedding_matrix, n_labels: int, \n",
    "    optimizer, dropout = 0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.n_features = n_features\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.n_labels = n_labels\n",
    "        # if we want to predict emoji vecs instead of emoji labels, use cosine proximity\n",
    "        self.loss = \"categorical_crossentropy\" \n",
    "        self.optimizer = optimizer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        print('Build model...')\n",
    "        self.model = Sequential()\n",
    "                \n",
    "        self.model.add(Embedding(vocab_size + 2, n_features, weights = [embedding_matrix], \n",
    "                                 trainable = False, mask_zero = True))\n",
    "\n",
    "\n",
    "        #self.model.add(LSTM(hidden_units, activation = 'relu', recurrent_activation = 'hard_sigmoid',\n",
    "                            #return_sequences = True))\n",
    "\n",
    "        #self.model.add(Dropout(self.dropout))\n",
    "\n",
    "        self.model.add(LSTM(hidden_units, activation = 'relu', \n",
    "                            recurrent_activation = 'hard_sigmoid', return_sequences = False))\n",
    "\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "\n",
    "        #self.model.add(TimeDistributed(Dense(self.n_labels, activation = 'softmax')))\n",
    "        self.model.add(Dense(self.n_labels, activation = 'softmax'))\n",
    "        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = ['accuracy'])\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val, n_epochs, n_batches):\n",
    "        return self.model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = n_epochs, batch_size = n_batches)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:02:37.194367Z",
     "start_time": "2019-05-17T18:02:37.191359Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "n_batches = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:03:15.613882Z",
     "start_time": "2019-05-17T18:03:11.735279Z"
    }
   },
   "outputs": [],
   "source": [
    "LSTM_NN = LSTM_NET(vocab_size, hidden_units, n_features, embedding_mat, n_labels, optimizer, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:01:13.641034Z",
     "start_time": "2019-05-17T18:01:00.637Z"
    }
   },
   "outputs": [],
   "source": [
    "LSTM_NN.fit(X_train, y_train, X_val, y_val, n_epochs, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:05:53.493746Z",
     "start_time": "2019-05-17T18:05:46.982600Z"
    }
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_pred_test = LSTM_NN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for tweet, pred in zip(cleaned_test_data, y_pred_labels)\n",
    "    print(tweet)\n",
    "    print()\n",
    "    print(idx_emoji[pred])\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:23:04.490523Z",
     "start_time": "2019-05-17T17:23:04.372208Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:06:26.427382Z",
     "start_time": "2019-05-17T18:06:26.399307Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert predictions to labels\n",
    "y_pred_labels = preds_to_labels(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:19:29.837610Z",
     "start_time": "2019-05-17T18:19:29.820564Z"
    }
   },
   "outputs": [],
   "source": [
    "freq = {}\n",
    "for pred in y_pred_labels:\n",
    "    if idx_emoji[pred] in freq:\n",
    "        freq[idx_emoji[pred]] += 1\n",
    "    else:\n",
    "        freq[idx_emoji[pred]] = 1 \n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:12:00.999999Z",
     "start_time": "2019-05-17T18:12:00.985962Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for tweet, pred, true in zip(cleaned_test_data, y_pred_labels, test_labels):\n",
    "    print(tweet)\n",
    "    print(\"prediction:\", idx_emoji[pred])\n",
    "    print(\"true label:\", true)\n",
    "    print()\n",
    "    if i == 30:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:23:43.191645Z",
     "start_time": "2019-05-17T17:22:09.866Z"
    }
   },
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_labels, average = 'micro')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "588.496px",
    "left": "2030px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
