{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:56:34.471076Z",
     "start_time": "2019-05-18T13:56:34.462052Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "import getopt\n",
    "import logging\n",
    "#import nltk\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tweepy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, SimpleRNN\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from RNNs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:40.032226Z",
     "start_time": "2019-05-18T11:50:39.139080Z"
    }
   },
   "outputs": [],
   "source": [
    "train_proc = pd.read_csv('train_set_processed.csv')\n",
    "val_proc = pd.read_csv('val_set_processed.csv')\n",
    "test_proc = pd.read_csv('test_set_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:41.090037Z",
     "start_time": "2019-05-18T11:50:41.081013Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def count_emojis(data, n = 10):\n",
    "    \"\"\"\n",
    "    Function that counts the number of emojis in the data set.\n",
    "    Display the n most frequent emojis.\n",
    "    \"\"\"\n",
    "    emoji_counts = {}\n",
    "    for index, row in data.iterrows():\n",
    "        emoji = row[1]\n",
    "        if emoji not in emoji_counts:\n",
    "            # compute simultaneous counting\n",
    "            emoji_counts[emoji] = data[data.label == emoji].count()[1]\n",
    "            \n",
    "    # sort emojis by freq in descending order (list of tuples will be returned)\n",
    "    sorted_emoji_counts = sorted(emoji_counts.items(), key= lambda kv: kv[1], reverse=True)\n",
    "        \n",
    "    return [emoji[0] for emoji in sorted_emoji_counts[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:50.545991Z",
     "start_time": "2019-05-18T11:50:43.809087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['üòç', 'üòÇ', '‚ù§Ô∏è', 'üíï', 'üòä', 'üòò', 'üò≠', 'üíñ', 'üòé', '‚ú®']\n"
     ]
    }
   ],
   "source": [
    "top_10_test = count_emojis(test_proc)\n",
    "print(top_10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:51.839004Z",
     "start_time": "2019-05-18T11:50:51.835996Z"
    }
   },
   "outputs": [],
   "source": [
    "### emoji map for top 10 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:52.377435Z",
     "start_time": "2019-05-18T11:50:52.373424Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_map = {emoji: i for i, emoji in enumerate(top_10_test)}\n",
    "idx_emoji = {i: emoji for i, emoji in enumerate(top_10_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:52.905839Z",
     "start_time": "2019-05-18T11:50:52.901828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['üòç', 'üòÇ', '‚ù§Ô∏è', 'üòä', 'üòò', 'üò≠', 'üòé', '‚ú®']\n"
     ]
    }
   ],
   "source": [
    "distinct_8 = [emoji for emoji, i in emoji_map.items() if i is not 3 and i is not 7]\n",
    "print(distinct_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:53.601574Z",
     "start_time": "2019-05-18T11:50:53.598567Z"
    }
   },
   "outputs": [],
   "source": [
    "### emoji map for distinct 8 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:54.106919Z",
     "start_time": "2019-05-18T11:50:54.101904Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_map = {emoji: i for i, emoji in enumerate(distinct_8)}\n",
    "idx_emoji = {i: emoji for i, emoji in enumerate(distinct_8)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:55.768976Z",
     "start_time": "2019-05-18T11:50:55.762959Z"
    }
   },
   "outputs": [],
   "source": [
    "def emoji_to_int(labels: list):\n",
    "    return [emoji_map[emoji] for emoji in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:50:56.347514Z",
     "start_time": "2019-05-18T11:50:56.340494Z"
    }
   },
   "outputs": [],
   "source": [
    "def distinct_eight(data, distinct_8: list): \n",
    "    \"\"\"\n",
    "    Function that checks, whether Tweet consists of one of the distinct eight emojis.\n",
    "    The distinct eight emojis are a subset of the ten most frequent emojis in the dataset,\n",
    "    where emojis with similar meaning were filtered.\n",
    "    If, and only if, Tweet consists one of those emojis, \n",
    "    Tweet will be used for further analysis.\n",
    "    Else: Line will be dropped.\n",
    "    \"\"\"\n",
    "    idx_drop = []\n",
    "    for index, row in data.iterrows():\n",
    "        if row[1] not in distinct_8:\n",
    "            idx_drop.append(index)\n",
    "    return data.drop(data.index[idx_drop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T08:33:45.531986Z",
     "start_time": "2019-05-18T08:33:45.526975Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def keep_top_10(data, top_10: list): \n",
    "    \"\"\"\n",
    "    Function that checks, whether Tweet consists of one of the top ten emojis.\n",
    "    If, and only if, Tweet consists one of the most frequent emojis, \n",
    "    Tweet will be used for further analysis.\n",
    "    Else: Line will be dropped.\n",
    "    \"\"\"\n",
    "    idx_drop = []\n",
    "    for index, row in data.iterrows():\n",
    "        if row[1] not in top_10:\n",
    "            idx_drop.append(index)\n",
    "    return data.drop(data.index[idx_drop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T08:34:24.363544Z",
     "start_time": "2019-05-18T08:34:24.360548Z"
    }
   },
   "outputs": [],
   "source": [
    "### distinct 8 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:51:46.219497Z",
     "start_time": "2019-05-18T11:50:59.209113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets in the train data set: 70054\n"
     ]
    }
   ],
   "source": [
    "train_data = distinct_eight(train_proc, distinct_8)\n",
    "print(\"Number of Tweets in the train data set: {}\".format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:51:51.487498Z",
     "start_time": "2019-05-18T11:51:46.381929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets in the test data set: 6536\n"
     ]
    }
   ],
   "source": [
    "test_data = distinct_eight(test_proc, distinct_8)\n",
    "print(\"Number of Tweets in the test data set: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:51:56.156109Z",
     "start_time": "2019-05-18T11:51:51.689033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets in the validation data set: 6536\n"
     ]
    }
   ],
   "source": [
    "val_data = distinct_eight(val_proc, distinct_8)\n",
    "print(\"Number of Tweets in the validation data set: {}\".format(len(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### top 10 ### (don't execute for now, maybe useful for comparison with distinct eight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T08:37:18.175144Z",
     "start_time": "2019-05-18T08:36:37.483869Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = keep_top_10(train_proc, top_10_test)\n",
    "print(\"Number of Tweets in the train data set: {}\".format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T08:37:22.905733Z",
     "start_time": "2019-05-18T08:37:18.259368Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = keep_top_10(test_proc, top_10_test)\n",
    "print(\"Number of Tweets in the test data set: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T08:37:26.161397Z",
     "start_time": "2019-05-18T08:37:22.988954Z"
    }
   },
   "outputs": [],
   "source": [
    "val_data = keep_top_10(val_proc, top_10_test)\n",
    "print(\"Number of Tweets in the validation data set: {}\".format(len(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:51:56.409783Z",
     "start_time": "2019-05-18T11:51:56.375694Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create list of stopwords\n",
    "stop_words = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:51:56.621345Z",
     "start_time": "2019-05-18T11:51:56.574221Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tweets_cleaning(tweets, labels, stopwords: list, train = False, use_bigrams = False, \n",
    "                    lowercase = True, stemming = False, min_df = 2, embedding = False):\n",
    "    \"\"\"\n",
    "    Text cleaning function that performs all necessary text preprocessing steps.\n",
    "    Function only keeps characters, that are alphanumerical (non-alphanumerical values are discarded).\n",
    "    Digits are treated by regular expressions.\n",
    "    Lower-casing is performed to reduce noise and normalize the text (convert it into a uniform representation).\n",
    "    Stemming is performed to only keep the stem of each word token but not any other deviated form. \n",
    "    Stop words (i.e., words that occur more frequently than other words in a given corpus) are removed.\n",
    "    \"\"\"\n",
    "    \n",
    "     # initialize Lancaster stemmer\n",
    "    if stemming:\n",
    "        st = LancasterStemmer()\n",
    "    \n",
    "    cleaned_data = []\n",
    "    cleaned_labels = []\n",
    "    \n",
    "    all_bigrams = [] # serves as place-holder\n",
    "    bigrams_dict = dict()\n",
    "    vocab = dict()\n",
    "    \n",
    "    for tweet, label in zip(tweets, labels):\n",
    "        tweet = re.sub(r'&amp\\S+','', tweet)\n",
    "        tweet = re.sub(r' & ', ' and ', tweet)\n",
    "        tweet = re.sub(r'!+', ' ! ', tweet)\n",
    "        tweet = re.sub(r'[?]+', ' ? ', tweet)\n",
    "        tweet = re.sub('@.+', '@user', tweet)\n",
    "        tweet = re.sub('#', '# ', tweet)\n",
    "\n",
    "        # Create spaces instead of some punctuation marks, but not if it's part of an emoticon\n",
    "        tweet = ' '.join([word if re.search(r'(?:X|:|;|=)(?:-)?(?:\\)|\\(|O|D|P|S)+', word)\n",
    "            else re.sub('[,.;\\-_:/\\n\\t]+', ' ', word) for word in tweet.split()])\n",
    "        \n",
    "        tweet = tweet.split(\" \")\n",
    "        \n",
    "        cleaned_tweet = []\n",
    "        for word in tweet:\n",
    "            \n",
    "            #if emoticon is in word, keep the emoticon\n",
    "            if re.search(r'(?:X|:|;|=)(?:-)?(?:\\)|\\(|O|D|P|S)+', word):\n",
    "                cleaned_word = word\n",
    "            else:\n",
    "                # keep special characters which might carry important information\n",
    "                # perform lower-casing to normalize the text and reduce noise\n",
    "                cleaned_word = ''.join([char for char in word if re.search('[<>$#‚Ç¨¬£!?@=]', char) or\n",
    "                                        char.isalnum()])\n",
    "            if lowercase:\n",
    "                cleaned_word = cleaned_word.lower()\n",
    "                \n",
    "            if \"<3\" not in cleaned_word:\n",
    "                cleaned_word = re.sub('[0-9]', '0', cleaned_word)\n",
    "  \n",
    "            # removes each \\n (i.e., new line) or \\t (i.e., tab) -> pipe char denotes a disjunction\n",
    "            cleaned_word = re.sub(r'( \\n| \\t)+', '', cleaned_word)\n",
    "            \n",
    "            if stemming:\n",
    "                cleaned_word = st.stem(cleaned_word)\n",
    "            \n",
    "            if len(cleaned_word) > 0 and cleaned_word not in stopwords:\n",
    "                cleaned_tweet.append(cleaned_word)\n",
    "                \n",
    "                if train:\n",
    "                    if cleaned_word in vocab:\n",
    "                        vocab[cleaned_word] += 1\n",
    "                    else:\n",
    "                        vocab[cleaned_word] = 1\n",
    "            \n",
    "        # only append tweets with more than 1 word per tweet\n",
    "        if len(cleaned_tweet) > 1:\n",
    "            \n",
    "            if train and use_bigrams:\n",
    "                \n",
    "                bigrams = [' '.join([cleaned_tweet[i-1], cleaned_tweet[i]]) \n",
    "                           for i, _ in enumerate(cleaned_tweet) if i > 0]\n",
    "                \n",
    "                for bigram in bigrams:\n",
    "                    \n",
    "                    if bigram in bigrams_dict:\n",
    "                        bigrams_dict[bigram] += 1\n",
    "                    else:\n",
    "                        bigrams_dict[bigram] = 1 \n",
    "\n",
    "            cleaned_tweet = ' '.join(cleaned_tweet)\n",
    "            cleaned_data.append(cleaned_tweet)\n",
    "            cleaned_labels.append(label)\n",
    "            \n",
    "    if train and embedding and not use_bigrams:\n",
    "        \n",
    "        word2index = dict()\n",
    "        i = 1\n",
    "        for word in vocab.keys():\n",
    "            word2index[word] = i\n",
    "            i += 1\n",
    "            \n",
    "        word2index.update({'UNK': len(word2idx) + 1})\n",
    "        \n",
    "        assert len(cleaned_data) == len(cleaned_labels)\n",
    "\n",
    "        return cleaned_data, cleaned_labels, word2index\n",
    "                \n",
    "    if train:\n",
    "        vocab = [word for word, freq in vocab.items() if freq >= min_df]\n",
    "        \n",
    "        if use_bigrams:\n",
    "            all_bigrams = [bigram for bigram, freq in bigrams_dict.items() if freq >= min_df]\n",
    "            vocab.extend(all_bigrams)\n",
    "            \n",
    "    \n",
    "    assert len(cleaned_data) == len(cleaned_labels)\n",
    "    \n",
    "    return cleaned_data, cleaned_labels, sorted(vocab), sorted(all_bigrams)\n",
    "\n",
    "# Potential problem: if min_df > 1, then maybe we should also filter all tweets that have less than 2 words of the vocab???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:52:12.876195Z",
     "start_time": "2019-05-18T11:51:56.812855Z"
    }
   },
   "outputs": [],
   "source": [
    "lower = True\n",
    "\n",
    "cleaned_train_data, train_labels, vocab, bigrams = tweets_cleaning(train_data.text, \n",
    "                                                                   train_data.label, \n",
    "                                                                   stop_words, \n",
    "                                                                   train = True, \n",
    "                                                                   use_bigrams = True, \n",
    "                                                                   lowercase = lower,\n",
    "                                                                   min_df = 2)\n",
    "\n",
    "cleaned_test_data, test_labels, _, _ = tweets_cleaning(test_data.text, \n",
    "                                                       test_data.label, \n",
    "                                                       stop_words, \n",
    "                                                       lowercase = lower)\n",
    "\n",
    "cleaned_val_data, val_labels, _, _ = tweets_cleaning(val_data.text, \n",
    "                                                     val_data.label, \n",
    "                                                     stop_words, \n",
    "                                                     lowercase = lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:52:14.165283Z",
     "start_time": "2019-05-18T11:52:14.155257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets per data set after text cleaning was computed:\n",
      "\n",
      "Train: 55299\n",
      "\n",
      "Test: 5237\n",
      "\n",
      "Validation: 5219\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Tweets per data set after text cleaning was computed:\")\n",
    "print()\n",
    "print(\"Train: {}\".format(len(cleaned_train_data)))\n",
    "print()\n",
    "print(\"Test: {}\".format(len(cleaned_test_data)))\n",
    "print()\n",
    "print(\"Validation: {}\".format(len(cleaned_val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:52:18.983657Z",
     "start_time": "2019-05-18T11:52:14.351779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the vocabulary: 33200\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique tokens in the vocabulary: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:52:19.377703Z",
     "start_time": "2019-05-18T11:52:19.298495Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = emoji_to_int(train_labels)\n",
    "y_test = emoji_to_int(test_labels)\n",
    "y_val = emoji_to_int(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for the Bag of Words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:52:41.208202Z",
     "start_time": "2019-05-18T11:52:41.197172Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bag_of_words(train: list, test: list, val: list, ngram: tuple, vocab = None, \n",
    "                 n_best_factor = 0.7):\n",
    "    \"\"\"\n",
    "    Create a weighted bag-of-words unigram or bigram representation of provided tweets.\n",
    "    Ngram is set to unigram by default. If bigram bag-of-words should be created, pass tuple (2, 2).\n",
    "    \n",
    "    Vocabulary argument is set to None by default. \n",
    "    You can pass a vocabulary to this function, which may then be used for TfidfVectorizer. \n",
    "    If you do not pass a vocabulary to this function, TfidfVectorizer will create a vocabulary itself.\n",
    "    \"\"\" \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(encoding = 'utf-8', ngram_range = ngram, analyzer = 'word', \n",
    "                                 vocabulary = vocab, min_df = 0.1, max_df = 0.9, norm = 'l2',\n",
    "                                 smooth_idf = True, sublinear_tf = False)\n",
    "    \n",
    "    train_BoW = vectorizer.fit_transform(train).toarray()\n",
    "    test_BoW = vectorizer.transform(test).toarray()\n",
    "    val_BoW = vectorizer.transform(val).toarray()\n",
    "    \n",
    "    #n_best = int(len(vectorizer.idf_) * n_best_factor)\n",
    "    #idx = np.argsort(vectorizer.idf_)[:n_best]\n",
    "\n",
    "    #train_BoW = train_BoW[:, idx]\n",
    "    #test_BoW = test_BoW[:, idx]\n",
    "    #val_BoW = val_BoW[:, idx]\n",
    "\n",
    "\n",
    "    return train_BoW, test_BoW, val_BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:52:45.463903Z",
     "start_time": "2019-05-18T11:52:45.457886Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_cat_matrix(y):\n",
    "\n",
    "    \"\"\" \n",
    "    Binary one-hot encoding using an indicator matrix.\n",
    "    This function converts labels to a categorical matrix which is of size N x K.\n",
    "    Each row is a row vector with k-1 zeros and a single 1.\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    K = len(set(y))\n",
    "    ind_matrix = np.zeros((N,K), dtype = int)\n",
    "    \n",
    "    for i, cat in enumerate(y):\n",
    "        ind_matrix[i, int(cat)] = 1\n",
    "        \n",
    "    return ind_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:53:48.654076Z",
     "start_time": "2019-05-18T11:53:46.262708Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, X_val = bag_of_words(cleaned_train_data, cleaned_test_data, cleaned_val_data, ngram = (1, 2), vocab = vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T14:12:28.356407Z",
     "start_time": "2019-05-09T14:10:44.195Z"
    }
   },
   "outputs": [],
   "source": [
    "#np.savetxt('X_train_BoW.txt', X_train)\n",
    "#np.savetxt('X_test_BoW.txt', X_test)\n",
    "#np.savetxt('X_val_BoW.txt', X_val)\n",
    "\n",
    "#np.savetxt('y_train_BoW.txt', y_train)\n",
    "#np.savetxt('y_test_BoW.txt', y_test)\n",
    "#np.savetxt('y_val_BoW.txt', y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network (Multilayer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:38:21.322067Z",
     "start_time": "2019-05-18T13:38:21.311039Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(hidden_units: int, input_dims: int, n_labels: int):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, input_dim = input_dims, activation = 'relu'))\n",
    "    model.add(Dropout(0.5)) # dropout is important to prevent model from overfitting\n",
    "    model.add(Dense(n_labels, activation = 'softmax'))\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1 = 0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:54:28.002685Z",
     "start_time": "2019-05-18T11:53:54.941610Z"
    }
   },
   "outputs": [],
   "source": [
    "def preds_to_labels(ypred):\n",
    "    \"\"\"\n",
    "    Firstly, extract the predicted label from a vector of probability distributions.\n",
    "    Secondly, retrieve index of highest value (i.e., highest probability).\n",
    "    \"\"\"\n",
    "    num_labels = [np.argmax(pred) for pred in ypred]\n",
    "    return np.array(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:38:26.981394Z",
     "start_time": "2019-05-18T13:38:26.978387Z"
    }
   },
   "outputs": [],
   "source": [
    "# set number of hidden units, epochs and batch size\n",
    "n_units = 60\n",
    "n_epochs = 5\n",
    "n_batches = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:55:02.324457Z",
     "start_time": "2019-05-18T11:54:54.513324Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-1458bbae4ace>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# shuffle data before fitting the neural network with it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36mshuffle\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \"\"\"\n\u001b[0;32m    402\u001b[0m     \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m     \u001b[0mresampled_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msafe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m     \u001b[0mresampled_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msafe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    214\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[0;32m    215\u001b[0m             \u001b[1;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# shuffle data before fitting the neural network with it\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_val, y_val = shuffle(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T11:54:41.766098Z",
     "start_time": "2019-05-18T11:54:41.741032Z"
    }
   },
   "outputs": [],
   "source": [
    "# get indicator matrix with one-hot-encoded vectors per label (of all labels)\n",
    "y_train = to_cat_matrix(y_train)\n",
    "y_val = to_cat_matrix(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:38:35.235459Z",
     "start_time": "2019-05-18T13:38:34.571399Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_model(n_units, X_train.shape[1], y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:38:40.180314Z",
     "start_time": "2019-05-18T13:38:40.175302Z"
    }
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:52:57.015047Z",
     "start_time": "2019-05-18T13:38:41.252164Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55299 samples, validate on 5219 samples\n",
      "Epoch 1/5\n",
      "55299/55299 [==============================] - 163s 3ms/step - loss: 1.7691 - acc: 0.3654 - val_loss: 1.5997 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.44472, saving model to best_model.h5\n",
      "Epoch 2/5\n",
      "55299/55299 [==============================] - 171s 3ms/step - loss: 1.5033 - acc: 0.4762 - val_loss: 1.5151 - val_acc: 0.4656\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.44472 to 0.46561, saving model to best_model.h5\n",
      "Epoch 3/5\n",
      "55299/55299 [==============================] - 170s 3ms/step - loss: 1.3168 - acc: 0.5451 - val_loss: 1.4972 - val_acc: 0.4779\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.46561 to 0.47787, saving model to best_model.h5\n",
      "Epoch 4/5\n",
      "55299/55299 [==============================] - 180s 3ms/step - loss: 1.1677 - acc: 0.5962 - val_loss: 1.5185 - val_acc: 0.4781\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.47787 to 0.47806, saving model to best_model.h5\n",
      "Epoch 5/5\n",
      "55299/55299 [==============================] - 166s 3ms/step - loss: 1.0478 - acc: 0.6390 - val_loss: 1.5518 - val_acc: 0.4813\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.47806 to 0.48132, saving model to best_model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2246d85f128>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = n_epochs, \n",
    "          batch_size = n_batches, callbacks = [es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:53:43.833401Z",
     "start_time": "2019-05-18T13:53:36.040408Z"
    }
   },
   "outputs": [],
   "source": [
    "# load best model\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:53:50.001937Z",
     "start_time": "2019-05-18T13:53:46.965366Z"
    }
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_pred_test = saved_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:53:50.422063Z",
     "start_time": "2019-05-18T13:53:50.411025Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert predictions to labels\n",
    "y_pred_labels = preds_to_labels(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T14:50:38.009057Z",
     "start_time": "2019-05-18T14:50:38.003041Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_top_n(y_true, y_preds, top_n = 3):\n",
    "    \"\"\"\n",
    "    If the correct label / emoji is among the top n (e.g., two, three) predictions,\n",
    "    we consider the prediction as correctly labeled.\n",
    "    \"\"\"\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    \n",
    "    for i, pred in enumerate(y_preds):\n",
    "        top_3 = np.argsort(pred)[-top_n:]\n",
    "        if y_true[i] in top_3:\n",
    "            n_correct += 1\n",
    "        n_total += 1\n",
    "        \n",
    "    ratio = n_correct / n_total\n",
    "    return round(ratio, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T14:52:22.495068Z",
     "start_time": "2019-05-18T14:52:22.414856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8457"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if true label is among the top 4 predictions, prediction is deemed correctly labeled\n",
    "accuracy_top_n(y_test, y_pred_test, top_n = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T14:50:50.737819Z",
     "start_time": "2019-05-18T14:50:50.634544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7718"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if true label is among the top 3 predictions, prediction is deemed correctly labeled\n",
    "accuracy_top_n(y_test, y_pred_test, top_n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T14:52:14.304346Z",
     "start_time": "2019-05-18T14:52:14.213104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6664"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if true label is among the top 2 predictions, prediction is deemed correctly labeled\n",
    "accuracy_top_n(y_test, y_pred_test, top_n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:58:15.993925Z",
     "start_time": "2019-05-18T13:58:15.985904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47870918464769907"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:53:51.402523Z",
     "start_time": "2019-05-18T13:53:50.652281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46210658755042305"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred_labels, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:53:51.797569Z",
     "start_time": "2019-05-18T13:53:51.786541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47870918464769907"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred_labels, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:55:03.628619Z",
     "start_time": "2019-05-18T13:55:03.616588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank 00k\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: ‚ú®\n",
      "\n",
      "asked chocolate last night didnt got today blessed\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: üòä\n",
      "\n",
      "# ewepvtltd # dubai ! besttt @user\n",
      "prediction: üòç\n",
      "true label: ‚ù§Ô∏è\n",
      "\n",
      "weekend norman hosting 00rd annual jazz june music festival showcasing best jazz around !\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: üòé\n",
      "\n",
      "picture full trash expect like\n",
      "prediction: üòÇ\n",
      "true label: üò≠\n",
      "\n",
      "pray kelso\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: üòÇ\n",
      "\n",
      "get girl\n",
      "prediction: üòç\n",
      "true label: üòç\n",
      "\n",
      "happy birthday sweetest girl im blessed call best friend ilysm\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: ‚ù§Ô∏è\n",
      "\n",
      "im love\n",
      "prediction: üòç\n",
      "true label: üòç\n",
      "\n",
      "look like fat cameltoe\n",
      "prediction: üòÇ\n",
      "true label: üòÇ\n",
      "\n",
      "good last couple days witchoo\n",
      "prediction: ‚ú®\n",
      "true label: ‚ú®\n",
      "\n",
      "flashback friday\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: ‚ù§Ô∏è\n",
      "\n",
      "spent two hours life\n",
      "prediction: üòç\n",
      "true label: üòÇ\n",
      "\n",
      "love stance\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: üòç\n",
      "\n",
      "tasmanian devils need appreciation\n",
      "prediction: üòç\n",
      "true label: ‚ù§Ô∏è\n",
      "\n",
      "moment forever piss\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: üòÇ\n",
      "\n",
      "dead ass\n",
      "prediction: üòÇ\n",
      "true label: üòÇ\n",
      "\n",
      "happy birthday buddy hope good one @user\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: ‚ù§Ô∏è\n",
      "\n",
      "arrived paris\n",
      "prediction: üòç\n",
      "true label: ‚ù§Ô∏è\n",
      "\n",
      "suits dear\n",
      "prediction: üòç\n",
      "true label: üòÇ\n",
      "\n",
      "throwback time # hyuna\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: ‚ù§Ô∏è\n",
      "\n",
      "00 come little closer say\n",
      "prediction: üòç\n",
      "true label: üòÇ\n",
      "\n",
      "anybody attractive shining =gt course hes baby @user\n",
      "prediction: üòç\n",
      "true label: üòç\n",
      "\n",
      "right around corner @user\n",
      "prediction: üòç\n",
      "true label: üòç\n",
      "\n",
      "love blondie\n",
      "prediction: üòç\n",
      "true label: ‚ù§Ô∏è\n",
      "\n",
      "look fave cute\n",
      "prediction: üòç\n",
      "true label: ‚ù§Ô∏è\n",
      "\n",
      "seattle breath taking\n",
      "prediction: üòç\n",
      "true label: üòç\n",
      "\n",
      "sword omens master sword buster sword lightsaber\n",
      "prediction: ‚ù§Ô∏è\n",
      "true label: üòç\n",
      "\n",
      "follow ig jaydaslaughter\n",
      "prediction: üòä\n",
      "true label: ‚ú®\n",
      "\n",
      "beautiful moment love game still remember goal\n",
      "prediction: üòÇ\n",
      "true label: üòç\n",
      "\n",
      "lucky im love best friend\n",
      "prediction: üòç\n",
      "true label: ‚ù§Ô∏è\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for tweet, pred, true in zip(cleaned_test_data, y_pred_labels, test_labels):\n",
    "    print(tweet)\n",
    "    print(\"prediction:\", idx_emoji[pred])\n",
    "    print(\"true label:\", true)\n",
    "    print()\n",
    "    if i == 30:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:54:05.379606Z",
     "start_time": "2019-05-18T13:54:05.368577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'‚ù§Ô∏è': 1248, 'üòç': 1631, 'üòÇ': 1330, '‚ú®': 206, 'üòä': 369, 'üò≠': 159, 'üòé': 173, 'üòò': 121}\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for pred in y_pred_labels:\n",
    "    if idx_emoji[pred] in freq:\n",
    "        freq[idx_emoji[pred]] += 1\n",
    "    else:\n",
    "        freq[idx_emoji[pred]] = 1 \n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-18T13:54:06.444436Z",
     "start_time": "2019-05-18T13:54:06.436417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'‚ú®': 310, 'üòä': 495, '‚ù§Ô∏è': 1130, 'üòé': 294, 'üò≠': 315, 'üòÇ': 1112, 'üòç': 1270, 'üòò': 311}\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "for y_true in y_test:\n",
    "    if idx_emoji[y_true] in freq:\n",
    "        freq[idx_emoji[y_true]] += 1\n",
    "    else:\n",
    "        freq[idx_emoji[y_true]] = 1 \n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT PART ONLY FOR RESEARCH PAPER BUT NOT FOR COGSCI II PROJECT !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:54:48.328807Z",
     "start_time": "2019-05-17T11:54:33.275750Z"
    }
   },
   "outputs": [],
   "source": [
    "lower = True\n",
    "\n",
    "cleaned_train_data, train_labels, word2idx = tweets_cleaning(train_data.text, \n",
    "                                                                   train_data.label, \n",
    "                                                                   stop_words, \n",
    "                                                                   train = True, \n",
    "                                                                   use_bigrams = False, \n",
    "                                                                   lowercase = lower,\n",
    "                                                                   min_df = 2,\n",
    "                                                                   embedding = True)\n",
    "\n",
    "cleaned_test_data, test_labels, _, _ = tweets_cleaning(test_data.text, \n",
    "                                                       test_data.label, \n",
    "                                                       stop_words, \n",
    "                                                       lowercase = lower)\n",
    "\n",
    "cleaned_val_data, val_labels, _, _ = tweets_cleaning(val_data.text, \n",
    "                                                     val_data.label, \n",
    "                                                     stop_words, \n",
    "                                                     lowercase = lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T14:37:27.826469Z",
     "start_time": "2019-05-17T14:37:27.743248Z"
    }
   },
   "outputs": [],
   "source": [
    "# only convert y_train and y_val to categorical matrix\n",
    "y_train = to_cat_matrix(emoji_to_int(train_labels))\n",
    "y_val = to_cat_matrix(emoji_to_int(val_labels))\n",
    "\n",
    "y_test = emoji_to_int(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:56:24.593461Z",
     "start_time": "2019-05-17T11:56:24.584438Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent2idx(word2idx: dict, documents: list):\n",
    "    \n",
    "    idx_docs = list()\n",
    "    max_length = max([len(document) for document in documents])\n",
    "    \n",
    "    for document in documents: \n",
    "        idx_doc = [word2idx[word] if word in word2idx else word2idx['UNK'] \n",
    "                   for word in document.split()]\n",
    "        \n",
    "        if len(idx_doc) < max_length:\n",
    "            idx_doc.extend([0 for _ in range(max_length - len(idx_doc))])\n",
    "            \n",
    "        idx_docs.append(idx_doc)\n",
    "        \n",
    "    return np.array(idx_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:01.794452Z",
     "start_time": "2019-05-17T11:57:00.331560Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = sent2idx(word2idx, cleaned_train_data)\n",
    "X_val = sent2idx(word2idx, cleaned_val_data)\n",
    "X_test = sent2idx(word2idx, cleaned_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data before fitting the neural network with it\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_val, y_val = shuffle(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T12:10:54.221067Z",
     "start_time": "2019-05-17T12:10:54.212043Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_embeddings(text_file, dim):\n",
    "\n",
    "    \"\"\" \n",
    "    Read GloVe txt.-file, load pre-trained word embeddings into memory\n",
    "    and create a word_to_embedding dictionary, where keys are the discrete word strings\n",
    "    and values are the corresponding continuous word embeddings, retrieved from the GloVe txt.-file.\n",
    "    For unkown words, the representation is an empty vector (i.e., zeros matrix).\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    with open(text_file, encoding=\"utf8\") as file:\n",
    "\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            wordvec = np.array(values[1:], dtype = 'float32')\n",
    "            embeddings_dict[word] = list(wordvec)\n",
    "    \n",
    "    embeddings_dict.update({'UNK': [0 for _ in range(dim)]})\n",
    "\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:27:42.010931Z",
     "start_time": "2019-03-28T14:27:41.661287Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_embeddings = get_embeddings(\"emoji2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T14:27:57.066502Z",
     "start_time": "2019-03-28T14:27:57.057477Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_emojivecs(emoji_embeddings: dict, corpus: list, dims: int):\n",
    "\n",
    "    N = len(corpus)\n",
    "    M = dims\n",
    "    \n",
    "    emojivecs = []\n",
    "    \n",
    "    # document = tweet; corpus = all tweets\n",
    "    for emoji in corpus:\n",
    "        emoji_sequence = []\n",
    "\n",
    "        try:\n",
    "            emojivec = emoji_embeddings[emoji]\n",
    "            assert len(emojivec) == M\n",
    "            emoji_sequence.append(emojivec)\n",
    "        except KeyError:\n",
    "            emoji_sequence.append([0 for _ in range(M)])\n",
    "            print(\"This {} does not exist in the pre-trained emoji embeddings.\".format(emoji))\n",
    "\n",
    "        emojivecs.append(emoji_sequence)\n",
    "\n",
    "    assert len(emojivecs) == N\n",
    "    return np.array(emojivecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_wordvecs(word_embeddings: dict, corpus: list, dims: int, zeros_padding = False):\n",
    "\n",
    "    \"\"\" \n",
    "    Return a concatenated word vector representation of each tweet.\n",
    "    The concatenated word vectors serve as the input data for the LSTM RNN.\n",
    "    Each word (embedding) denotes a time step. (Number of timesteps is equal to the length of the input sentence.)\n",
    "    \n",
    "    Check whether length of word vector is equal to the number of dimensions we pass to this function.\n",
    "    For unknown words (i.e., if key does not exist), the representation is an empty vector / zeros matrix of len dims.\n",
    "\n",
    "    Sequences can have variable length (i.e., number of time steps per batch).\n",
    "    However, in some cases you might want to zero pad the batch if a sequence < max length of sequences in the corpus.\n",
    "    By default this argument is set to False as Keras and Tensorflow except input sequences of variable length.\n",
    "    If set to True, zero padding is computed.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(corpus)\n",
    "    M = dims\n",
    "    global max_length\n",
    "    max_length = max([len(sequence) for sequence in corpus])\n",
    "    wordvecs_corpus = []\n",
    "    \n",
    "    # document = tweet; corpus = all tweets\n",
    "    for document in corpus:\n",
    "        wordvec_sequence = []\n",
    "        for word in document:\n",
    "            \n",
    "            try:\n",
    "                wordvec = word_embeddings[word]\n",
    "                assert len(wordvec) == M\n",
    "                wordvec_sequence.append(wordvec)\n",
    "            except KeyError:\n",
    "                wordvec_sequence.append([0 for _ in range(M)])\n",
    "                \n",
    "        # needs to be resolved (!)\n",
    "        if zeros_padding == True: \n",
    "            if len(document) < max_length:\n",
    "\n",
    "                for _ in range(len(document), max_length):\n",
    "                    wordvec_sequence.append([0 for _ in range(M)])\n",
    "\n",
    "                assert len(wordvec_sequence) == max_length\n",
    "        wordvecs_corpus.append(wordvec_sequence)\n",
    "\n",
    "    assert len(wordvecs_corpus) == N\n",
    "    return np.array(wordvecs_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T12:14:13.632312Z",
     "start_time": "2019-05-17T12:14:13.622288Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding_matrix(word2idx: dict, embeddings_dict: dict, dim: int):\n",
    "    \n",
    "    embedding_mat = np.zeros((len(word2idx) + 2, dim))\n",
    "    \n",
    "    for word, idx in word2idx.items():\n",
    "        vec = embeddings_dict.get(word)\n",
    "        # if word is not found in embeddings dictionary, vector will be all zeros\n",
    "        if vec is not None:\n",
    "            embedding_mat[idx] = vec\n",
    "            \n",
    "    return embedding_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T12:11:48.526604Z",
     "start_time": "2019-05-17T12:11:29.939513Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_embeddings = get_embeddings(\"glove.6B.50d.txt\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T12:14:49.561920Z",
     "start_time": "2019-05-17T12:14:49.348352Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_mat = embedding_matrix(word2idx, word_embeddings, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:35:18.790718Z",
     "start_time": "2019-05-17T16:35:18.751614Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)\n",
    "hidden_units = 50\n",
    "n_features = 50\n",
    "n_labels = 10\n",
    "optimizer = keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, \n",
    "                                  decay = 0.0, amsgrad = False)\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:36:19.662697Z",
     "start_time": "2019-05-17T16:36:19.634621Z"
    }
   },
   "outputs": [],
   "source": [
    "class GRU_NET():\n",
    "\n",
    "    #embedding_dim = 300\n",
    "    \n",
    "    def __init__(self, vocab_size: int, hidden_units: int, n_features: int, embedding_matrix, \n",
    "                 n_labels: int, optimizer, dropout = 0.1):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.n_features = n_features\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.n_labels = n_labels\n",
    "        # if we want to predict emoji vecs instead of emoji labels, use cosine proximity\n",
    "        self.loss = \"categorical_crossentropy\" \n",
    "        self.optimizer = optimizer\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        print('Build model...')\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        self.model.add(Embedding(vocab_size + 2, n_features, weights = [embedding_matrix], \n",
    "                                 trainable = False, mask_zero = True))\n",
    "        \n",
    "        self.model.add(GRU(hidden_units, activation='relu', recurrent_activation='hard_sigmoid', \n",
    "                           return_sequences = True))    \n",
    "        \n",
    "        self.model.add(Dropout(dropout))\n",
    "        \n",
    "        self.model.add(GRU(hidden_units, activation='relu', recurrent_activation='hard_sigmoid', \n",
    "                           return_sequences = False))\n",
    "        \n",
    "        self.model.add(Dropout(dropout))\n",
    "        \n",
    "        #self.model.add(TimeDistributed(Dense(self.n_labels, activation = 'softmax')))\n",
    "        self.model.add(Dense(self.n_labels, activation = 'softmax'))\n",
    "        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = ['accuracy'])\n",
    "                       \n",
    "    def fit(self, X_train, y_train, X_val, y_val,  n_epochs, n_batches):\n",
    "        return self.model.fit(X_train, y_train, validation_data = (X_val, y_val), \n",
    "                              epochs = n_epochs, batch_size = n_batches)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:01:33.060710Z",
     "start_time": "2019-05-17T18:01:33.043663Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_NET():\n",
    "\n",
    "    #embedding_dim = 300\n",
    "\n",
    "    def __init__(self, vocab_size: int, hidden_units: int, n_features: int, embedding_matrix, n_labels: int, \n",
    "    optimizer, dropout = 0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.n_features = n_features\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.n_labels = n_labels\n",
    "        # if we want to predict emoji vecs instead of emoji labels, use cosine proximity\n",
    "        self.loss = \"categorical_crossentropy\" \n",
    "        self.optimizer = optimizer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        print('Build model...')\n",
    "        self.model = Sequential()\n",
    "                \n",
    "        self.model.add(Embedding(vocab_size + 2, n_features, weights = [embedding_matrix], \n",
    "                                 trainable = False, mask_zero = True))\n",
    "\n",
    "\n",
    "        #self.model.add(LSTM(hidden_units, activation = 'relu', recurrent_activation = 'hard_sigmoid',\n",
    "                            #return_sequences = True))\n",
    "\n",
    "        #self.model.add(Dropout(self.dropout))\n",
    "\n",
    "        self.model.add(LSTM(hidden_units, activation = 'relu', \n",
    "                            recurrent_activation = 'hard_sigmoid', return_sequences = False))\n",
    "\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "\n",
    "        #self.model.add(TimeDistributed(Dense(self.n_labels, activation = 'softmax')))\n",
    "        self.model.add(Dense(self.n_labels, activation = 'softmax'))\n",
    "        self.model.compile(loss = self.loss, optimizer = self.optimizer, metrics = ['accuracy'])\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val, n_epochs, n_batches):\n",
    "        return self.model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = n_epochs, batch_size = n_batches)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:02:37.194367Z",
     "start_time": "2019-05-17T18:02:37.191359Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "n_batches = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:03:15.613882Z",
     "start_time": "2019-05-17T18:03:11.735279Z"
    }
   },
   "outputs": [],
   "source": [
    "LSTM_NN = LSTM_NET(vocab_size, hidden_units, n_features, embedding_mat, n_labels, optimizer, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:01:13.641034Z",
     "start_time": "2019-05-17T18:01:00.637Z"
    }
   },
   "outputs": [],
   "source": [
    "LSTM_NN.fit(X_train, y_train, X_val, y_val, n_epochs, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:05:53.493746Z",
     "start_time": "2019-05-17T18:05:46.982600Z"
    }
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_pred_test = LSTM_NN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for tweet, pred in zip(cleaned_test_data, y_pred_labels)\n",
    "    print(tweet)\n",
    "    print()\n",
    "    print(idx_emoji[pred])\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:23:04.490523Z",
     "start_time": "2019-05-17T17:23:04.372208Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:06:26.427382Z",
     "start_time": "2019-05-17T18:06:26.399307Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert predictions to labels\n",
    "y_pred_labels = preds_to_labels(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:19:29.837610Z",
     "start_time": "2019-05-17T18:19:29.820564Z"
    }
   },
   "outputs": [],
   "source": [
    "freq = {}\n",
    "for pred in y_pred_labels:\n",
    "    if idx_emoji[pred] in freq:\n",
    "        freq[idx_emoji[pred]] += 1\n",
    "    else:\n",
    "        freq[idx_emoji[pred]] = 1 \n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:12:00.999999Z",
     "start_time": "2019-05-17T18:12:00.985962Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for tweet, pred, true in zip(cleaned_test_data, y_pred_labels, test_labels):\n",
    "    print(tweet)\n",
    "    print(\"prediction:\", idx_emoji[pred])\n",
    "    print(\"true label:\", true)\n",
    "    print()\n",
    "    if i == 30:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T17:23:43.191645Z",
     "start_time": "2019-05-17T17:22:09.866Z"
    }
   },
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_labels, average = 'micro')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "588.496px",
    "left": "2030px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
